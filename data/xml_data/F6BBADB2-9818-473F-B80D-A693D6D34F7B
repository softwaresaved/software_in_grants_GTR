<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/7A0397DD-E0C6-4EA3-8031-B841D2503C4D"><gtr:id>7A0397DD-E0C6-4EA3-8031-B841D2503C4D</gtr:id><gtr:name>Royal Holloway, University of London</gtr:name><gtr:department>Psychology</gtr:department><gtr:address><gtr:line1>Egham Hill</gtr:line1><gtr:line4>Egham</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>TW20 0EX</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/7A0397DD-E0C6-4EA3-8031-B841D2503C4D"><gtr:id>7A0397DD-E0C6-4EA3-8031-B841D2503C4D</gtr:id><gtr:name>Royal Holloway, University of London</gtr:name><gtr:address><gtr:line1>Egham Hill</gtr:line1><gtr:line4>Egham</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>TW20 0EX</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/A8AB4D99-6257-4691-A685-760851A7D21C"><gtr:id>A8AB4D99-6257-4691-A685-760851A7D21C</gtr:id><gtr:firstName>Karl</gtr:firstName><gtr:otherNames>John</gtr:otherNames><gtr:surname>Friston</gtr:surname><gtr:orcidId>0000-0001-7984-8909</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/5AA55F7C-FB63-4ED6-91C1-2197B0B993A0"><gtr:id>5AA55F7C-FB63-4ED6-91C1-2197B0B993A0</gtr:id><gtr:firstName>Nicholas</gtr:firstName><gtr:surname>Furl</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/31D382D5-AE31-473A-A1DF-24F20058A5F5"><gtr:id>31D382D5-AE31-473A-A1DF-24F20058A5F5</gtr:id><gtr:firstName>Richard</gtr:firstName><gtr:surname>Henson</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=ES%2FI01134X%2F2"><gtr:id>F6BBADB2-9818-473F-B80D-A693D6D34F7B</gtr:id><gtr:title>Neural Representation of the Identities and Expressions of Human Faces</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/I01134X/2</gtr:grantReference><gtr:abstractText>Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.</gtr:abstractText><gtr:fund><gtr:end>2015-12-01</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2014-06-02</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>61166</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Please note that ES/I01134X/1 and ES/I01134X/2 are the same grant, held by the PI when he was at different institutions. These have the same impacts.

The outputs of this grant were designed from the beginning to be primarily academic in nature. Although there are currently few documented demonstrations of influence outside of academia at present, there is potential that this grant will have indirect influence in the long term.

This grant addresses the issue of how the visual system recognises facial identities and expressions from dynamic videos. Understanding how emotional information is detected by the neural mechanisms in visual system may, in the long term, help those suffering disorders in face perception, such as prosopagnosia, or those suffering anxiety disorders, who are adversely affected when encountering emotional or threatening visual information.

As a result of this grant, our laboratory has developed a method for extracting and quantifying facial motion from video. We also have developed methods for animating computer rendered 3D models of faces with extracted motion data. These results have not yet been published. We have submitted one paper and other studies on these methods are ongoing. With further study, these methods may improve how automated systems process video data from faces. They may also create new graphics applications.

The grant has had a positive training outcome on Dr Furl's career, as he has transitioned from a soft money principal investigator at the MRC CBU to a permanent lectureship post at Royal Holloway, University of London.</gtr:description><gtr:id>05735972-5C20-425C-BA6D-DF2605B8F495</gtr:id><gtr:impactTypes/></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The brain is composed, in part, of &amp;quot;pathways&amp;quot;: connected networks of areas that represent information and perform an important function. The main objective of our ESRC grant was to explore whether there are multiple distinguishable brain pathways responsible for recognition of identities versus recognition of expressions and how these relate to neural representations of facial form and facial motion. We hypothesised that an analysis of brain responses to dynamic videos of facial movements would reveal new insight into this pathway structure in the brain. One of our aims was to use computational models (e.g., dynamic causal modelling, DCM) to test models of pathway structure against our data. 
Below I provide brief descriptions of the ESRC-funded studies that use dynamic faces and/or DCM methods to reveal these brain pathways, their facial form and motion representations, and how they can interact together when recognising facial expressions. We have also used connectivity models and methods to learn more about these pathways including (a) &amp;quot;face-blind&amp;quot; individuals, who are less able to recognise faces (developmental prosopagnosics), show deficits in how brain areas are connected, suggesting a facial identity-related pathway has been disrupted in these individuals; (b) brain areas that respond to faces use a characteristic &amp;quot;cross-frequency coupling&amp;quot; pattern to communicate, which may be a signature for the transmission of information in the face recognition pathways. Our grant is also associated with new studies, that remain ongoing and unpublished but are coming soon, and that further explore these topics.

Please note that part of this research was conducted at the MRC Cognition Brain Sciences Unit (CBU) in Cambridge and part of it was performed at Royal Holloway, University of London (RHUL). The work at these institutions derived from one grant project but are listed on ResearchFish as separate grants and so we have reported separate outputs for each. Here, we describe the details of outputs resulting from work at RHUL (ES/I01134X/2). Please see ResearchFish report ES/I01134X/1 for details of outputs resulting from the work conducted at the CBU at the CBU.


(1) Three of our research outputs associated with our work at the CBU (ES/I01134X/1) applied a type of connectivity modelling, DCM, to reveal the interactions associated with brain areas responding to form and motion. To better integrate these findings into the larger literature and wider theory, I wrote a review paper which covered applications of this type of connectivity modelling to understanding pathway organisation in the brain in the occipitotemporal cortex of the brain.

Furl N. 2015. Structural and effective connectivity among high-level visual areas. Frontiers in Human Neuroscience 9:253.


(2) DCM is a type of &amp;quot;effective&amp;quot; connectivity analysis. This means that DCM measures how functional responses in different brain areas affect each other. However, other methods can measure the physical &amp;quot;wiring&amp;quot; that connects different brain areas. We used one such method, diffusion tensor imaging (DTI) to measure how the brain areas that respond to faces are physically (structurally) connected. Moreover, we tested this &amp;quot;wiring structure&amp;quot; in typical individuals and developmental prosopagnosics, those who show deficient face recognition (of facial identities, at least). We found that one brain area, the fusiform face area, was not as well-connected in developmental prosopagnosics as in typical controls. This area may be key to recognising facial identity.

Song S, Garrido L, Nagy Z, Mohammadi S, Steel A, Driver J, Dolan RJ, Duchaine B, Furl N. 2015. Local but not long-range microstructural differences of the ventral temporal cortex in developmental prosopagnosia. Neuropsychologia 78:195-206.


(3) The third research output is in press and will be in print soon. So it is not yet listed among the relevant papers on ResearchFish. This studied used the same participants as in (2) to explore &amp;quot;effective&amp;quot; (rather than structural) connectivity. In other words, we used DCM to measure how face processing areas (including fusiform face area) influence each other's neural responses. This was measured with functional magnetic resonance imaging (fMRI, a type of brain imaging that creates detailed maps of brain responses to stimuli, such as faces). We were able to successfully fit a model to our data. In this model, brain responses that are specific to faces (compared to non-faces) arise because of the influence of connections between early visual cortex (where simple and local visual features are processed and represented) and areas dedicated to processing faces (such as fusiform face area and superior temporal sulcus). Importantly, the influence of these connections was reduced in developmental prosopagnosics (See 2). This analysis provides more evidence for the pathway involving facial identity recognition.

Lohse M., Duchaine B, Garrido L, Driver J, Dolan R &amp;amp; Furl N. In press. Effective Connectivity from Early Visual Cortex to Posterior Occipito-temporal Face Areas Predicts Developmental Prosopagnosia. J Neurosci</gtr:description><gtr:exploitationPathways>Our research has applications in developing artificial visual recognition systems for video information as well as developing clinical models of brain disorders. These studies suggest ways that abstract visual information can be coded by neurons as well as the computations these neurons perform when coding. This knowledge can be used to devise and improve artificial visual recognition systems. Particularly, our results using dynamic facial stimuli can help develop software which can visually recognise video. Our research using the macaque monkey will help develop animal models of visual function. Our research on oscillatory communication between brain regions is a first step towards developing sophisticated models of disorders such as schizophrenia.</gtr:exploitationPathways><gtr:id>0EB611AA-6070-49CC-AEFD-E4959C1C910C</gtr:id><gtr:sectors><gtr:sector>Education,Security and Diplomacy</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/24376331-F327-4013-B584-DAC3CADAA5B9"><gtr:id>24376331-F327-4013-B584-DAC3CADAA5B9</gtr:id><gtr:title>Structural and effective connectivity reveals potential network-based influences on category-sensitive visual areas.</gtr:title><gtr:parentPublicationTitle>Frontiers in human neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/b3e119c6de2dd7da3498604603acb757"><gtr:id>b3e119c6de2dd7da3498604603acb757</gtr:id><gtr:otherNames>Furl N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1662-5161</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/910BE8F0-E688-4E5B-B70A-73502998B9D8"><gtr:id>910BE8F0-E688-4E5B-B70A-73502998B9D8</gtr:id><gtr:title>Effective Connectivity from Early Visual Cortex to Posterior Occipitotemporal Face Areas Supports Face Selectivity and Predicts Developmental Prosopagnosia.</gtr:title><gtr:parentPublicationTitle>The Journal of neuroscience : the official journal of the Society for Neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/cbfa1a0e8bfddfcb2e3df45bfafe7533"><gtr:id>cbfa1a0e8bfddfcb2e3df45bfafe7533</gtr:id><gtr:otherNames>Lohse M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0270-6474</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/40CA07F4-A10E-4C7C-A08A-FA024BC83AF0"><gtr:id>40CA07F4-A10E-4C7C-A08A-FA024BC83AF0</gtr:id><gtr:title>Low-frequency oscillations employ a general coding of the spatio-temporal similarity of dynamic faces.</gtr:title><gtr:parentPublicationTitle>NeuroImage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/b3e119c6de2dd7da3498604603acb757"><gtr:id>b3e119c6de2dd7da3498604603acb757</gtr:id><gtr:otherNames>Furl N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1053-8119</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/E43E0718-0DB1-4482-8C36-952B292CAE41"><gtr:id>E43E0718-0DB1-4482-8C36-952B292CAE41</gtr:id><gtr:title>Local but not long-range microstructural differences of the ventral temporal cortex in developmental prosopagnosia.</gtr:title><gtr:parentPublicationTitle>Neuropsychologia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/dfdc2fd51eb5a3227aedcbfeea5248a7"><gtr:id>dfdc2fd51eb5a3227aedcbfeea5248a7</gtr:id><gtr:otherNames>Song S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0028-3932</gtr:issn></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">ES/I01134X/2</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>C905D2E0-3381-4086-93CD-E79ABF8501C1</gtr:id><gtr:grantRef>ES/I01134X/1</gtr:grantRef><gtr:amount>257979.56</gtr:amount><gtr:start>2011-09-26</gtr:start><gtr:end>2014-06-30</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>F6BBADB2-9818-473F-B80D-A693D6D34F7B</gtr:id><gtr:grantRef>ES/I01134X/2</gtr:grantRef><gtr:amount>61166.49</gtr:amount><gtr:start>2014-06-02</gtr:start><gtr:end>2015-12-01</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>