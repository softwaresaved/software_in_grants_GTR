<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/318B5D98-4CB4-4B10-A876-08FC93071A56"><gtr:id>318B5D98-4CB4-4B10-A876-08FC93071A56</gtr:id><gtr:name>King's College London</gtr:name><gtr:department>Informatics</gtr:department><gtr:address><gtr:line1>Capital House</gtr:line1><gtr:line2>2nd Floor, Guys Campus</gtr:line2><gtr:line3>42 Weston Street</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>SE1 3QD</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/318B5D98-4CB4-4B10-A876-08FC93071A56"><gtr:id>318B5D98-4CB4-4B10-A876-08FC93071A56</gtr:id><gtr:name>King's College London</gtr:name><gtr:address><gtr:line1>Capital House</gtr:line1><gtr:line2>2nd Floor, Guys Campus</gtr:line2><gtr:line3>42 Weston Street</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>SE1 3QD</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/19D96592-0E99-428B-9676-68272D5A3807"><gtr:id>19D96592-0E99-428B-9676-68272D5A3807</gtr:id><gtr:firstName>Matthew</gtr:firstName><gtr:otherNames>Jacob</gtr:otherNames><gtr:surname>Howard</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FP010202%2F1"><gtr:id>0F44A073-CE36-486E-95E5-597C30705AAA</gtr:id><gtr:title>Soft Robotic Skill Learning from Human Demonstration</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/P010202/1</gtr:grantReference><gtr:abstractText>SoftSkills will look at ways in which ordinary users can program robots way by teaching them how to control their movements and forces through demonstration. It will focus on using redundancy (additional degrees of freedom that are not directly required for a task) and compliance (the extent to which the robot gives way when subjected to external forces) in order to ensure safety and stability while teaching and interacting with robots, by blending behaviours taught to the robot, with specialist engineered controllers internal to the robot. In doing so, it is hoped that this will lead to new knowledge on how humans themselves exploit redundancy and compliance in their own musculoskeletal system, as well as enabling non expert users to program robots intuitively, leading to new applications in areas, such as manufacture, automation in the home and work place, transport, logistics, healthcare and agriculture.</gtr:abstractText><gtr:potentialImpactText>It is anticipated that advanced robotic systems will increasingly enter into interaction with humans and everyday objects and environments, with application in many areas, including manufacture, automation in the home and work place, transport, logistics, healthcare and agriculture. Key to their acceptance will be whether ordinary people are able to use them safely, in an intuitive way, to perform useful tasks. SoftSkills will examine how non expert users are able to program robots through demonstration, that is, by showing the robot what to do, rather than through traditional means such as coding in front of a computer. It will develop methods ensure the best performance out of these human robot interactions are by automatically blending user-programmed behaviours with expert controllers ensuring safety and stability. In the industrial domain, it is anticipated that the continuing rise of so called cobots, i.e., , collaborative robots that work in cooperation with human co-workers will, increasingly fuel economic demand for these enhanced robot capabilities. For example, enabling users to train robots skills in physically interacting with rigid/deformable surfaces opens up new domains of application e.g., service tasks such as wiping a table clean, medical tasks such as pressing soft tissues such as the skin of the abdomen to detect abnormalities, agricultural tasks such as running fingers over plants to inspect for diseases, among many others. Having these kinds of capabilities in a single, reprogrammable machine (as opposed to specialist, single purpose machinery), present an appealing business case to companies whose operations need to be dynamic in face of changes in consumer demands. Many do not invest in automation for the simple fact that consumer tastes change and products are continuously reinvented rendering specialist machinery obsolete. An adaptable, easy to use robotic system can help them improve productivity at lower labour cost, or enhance the working conditions of staff by relieving them of dangerous, dull or dirty tasks. An added benefit of facilitating the use of robots by ordinary users is that it may allow them to develop new robot uses, beyond the applications envisaged by engineers. The methods developed in SoftSkills are aimed at enabling users to program their own behaviours in an intuitive fashion, with the robot automatically compensating for things like safety and stability. This frees the user to flexibly experiment with the robot and find new tasks where it can be applied.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-03-31</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2017-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>92196</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">EP/P010202/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>53F27348-198B-4AEF-A34B-8307067F507C</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Systems engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>21CE2EA6-E7A2-4406-A045-0DA7CA19B695</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Control Engineering</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>