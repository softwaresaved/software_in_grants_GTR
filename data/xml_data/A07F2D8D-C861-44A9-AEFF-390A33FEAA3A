<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:department>Sch of Informatics</gtr:department><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/EF2CA888-3185-4370-B5C5-059F7685570B"><gtr:id>EF2CA888-3185-4370-B5C5-059F7685570B</gtr:id><gtr:firstName>Korin</gtr:firstName><gtr:surname>Richmond</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/6E9EAE62-9ADC-4E84-B950-56966D646A8F"><gtr:id>6E9EAE62-9ADC-4E84-B950-56966D646A8F</gtr:id><gtr:firstName>Simon</gtr:firstName><gtr:surname>King</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/5D7F708A-5218-40FC-BD20-E55CBA1B14F6"><gtr:id>5D7F708A-5218-40FC-BD20-E55CBA1B14F6</gtr:id><gtr:firstName>Steve</gtr:firstName><gtr:surname>Renals</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FE027741%2F1"><gtr:id>A07F2D8D-C861-44A9-AEFF-390A33FEAA3A</gtr:id><gtr:title>Data-driven articulatory modelling: foundations for a new generation of speech synthesis</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E027741/1</gtr:grantReference><gtr:abstractText>Technology to automatically generate artificial speech (speech synthesis) has come to sound natural enough within the past five years that its use has widened dramatically. Leaders in industry have integrated text-to-speech (TTS) systems into useful real-world applications, such as automated call-centres and call routing, telephone-based information systems (e.g. telephone banking or news services), readers for the visually impaired, and hands-free interfaces, such as car navigation systems.However, in spite of this success, state-of-the-art TTS systems are still severely limited in terms of control. In short, we can readily control what synthesisers say, but not how they say it. Therefore, although such systems are suitable for giving factual information in speech form, they are completely inadequate where a high level of expressiveness is required. By expressiveness we mean the ability to indicate questions or emphasis on selected words, or to convey emotion. Furthermore, the process of generating new synthetic voices is costly and labour-intensive. It is the aim of this project to develop an alternative to current speech synthesis technology with a comparable level of intelligibility and naturalness, but which affords far greater flexibility and control.Unit selection uses large collections of pre-recorded speech to perform synthesis by merely gluing together appropriate fragments in sequence. There is in effect little or no modelling of speech involved. In contrast, this project aims to develop a new model which is trained on pre-recorded speech and interprets it in a novel way: on the basis of its underlying articulation. The aim of this model is to produce synthetic speech which not only retains the qualities of the original speech used for training, but which also is much more versatile and therefore has the potential to be used in new and exciting ways.</gtr:abstractText><gtr:fund><gtr:end>2009-10-31</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-11-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>286856</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The speech synthesis techniques developed in this project have been integrated in the Festival and HTS open source toolkits.

The mngu0 corpus is a collection of articulatory data of different forms (electromagnetic articulography, 3D MRI, video, 3D scans of upper/lower jaw, audio etc.) acquired from one male British English speaker. This data provides a unique resource that shows how a typical talker's mouth is used when producing speech. On one hand, for example, the 3D MRI head scans together with the dental scans provide a detailed measurement of the anatomy of the speaker. Meanwhile, electromagnetic articulography captures dynamic movements of the speech articulators. In this technique, sensors are placed on the articulators (e.g. tongue, lips, jaw etc), and the movements of these points may be recorded. The mngu0 corpus contains the largest amount of speech recorded in this way in one sitting that is yet available (over an hour of continuous speech).Taken together, the different modalities of data in this corpus offer an unparalleled resource to support research into speech production and for developing multiple speech technology applications by incorporating knowledge of human speech production. This corpus has been released via a dedicated, forum-style web site under a licence that allows it to be used free of charge for research purposes.

Beneficiaries: Researchers working in the field of speech production research, and speech technology applications.</gtr:description><gtr:firstYearOfImpact>2010</gtr:firstYearOfImpact><gtr:id>6784D79E-8BF7-4C94-8057-6CEC31B012DA</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>At the time this project began, the dominant method for text-to-speech synthesis, whereby a computer is made to convert text to audible artificial speech, was called unit selection. This method relies on gluing together fragments of speech carefully chosen from several hours of recordings of a real human talking. The benefits of the approach are its simplicity and that it sounds exactly like the human that made the original recordings. The downsides, though, are the limited scope for changing the qualities of the synthesised speech, and the expense of making large numbers of high quality audio recordings for each new synthetic voice.

This project has pursued a different approach to synthesising speech, which is generally termed statistical parametric synthesis. Instead of merely glueing together pre-recorded snippets of speech, this approach applies powerful statistical models to examples of spoken sentences in order to learn how to produce new speech. For example, the model will learn how the underlying sounds of English combine to produce a word, and how words combine to produce a natural sounding sentence. Over the course of this project, this approach has rapidly gained popularity, and research in this direction has intensified around the world.

However, though the new statistical models indeed offer a great deal more flexibility in theory, in practice they are hugely complex and can be unwieldy to control, so exploiting that flexibility can be difficult. The major aim of this project has been to address this problem and to find ways to incorporate extra information into the statistical model, which can in turn be used to control and manipulate synthetic speech in a straightforward, transparent way. Specifically, we have sought to incorporate information about the human speech production mechanism (i.e. the articulators, such as the tongue, lips and jaw, and the vocal chords).

The most critical key findings of this project are the ways that have been demonstrated to show this may be successfully achieved. First, for example, it has been demonstrated that knowledge about the vibrations of the vocal chords can be incorporated in order to improve the voice quality of the synthesised speech, as well as offering explicit control over how the voice sounds. Second, it has been found possible to incorporate information about movements of the mouth, and then to control synthesis in terms of mouth movements. As examples of this control, we have demonstrated changing one sound to another, thus changing the identity of a word, for example changing bed to bad, simply by changing the position of the model's tongue. We have also shown it is possible to create speech sounds that are completely new and which match the general quality of the synthetic voice. This means the accent of the speech synthesiser may be modified, or foreign sounds may be incorporated seamlessly into the synthetic speech, allowing the synthesiser to speak in multiple languages and accents with the same voice.

Though this project has dealt primarily with articulatory data as the extra information, the general approach has since been expanded to work with other representations. For example, work is currently being undertaken to capture the noise environment to use as additional information for the synthesiser to use. When human talks in a noisy environment, they are known to change the way they speak. Ideally, a computer synthesiser will do the same to make synthetic speech more intelligible in varying noise conditions.

Beyond speech synthesis alone, an additional key finding of the project is a method to accurately predict articulatory movements from text. This is especially useful, for example, in applications such as animated talking heads and computer animation for films and games.

As a final key finding, the research work conducted during this project has confirmed both how useful articulatory data is, but also how difficult it is to collect in large quantities. Because of this, only small amounts of articulatory data have previously been released to the research community. Therefore, data recorded as part of this project has been released to share with other researchers worldwide at no cost.</gtr:description><gtr:exploitationPathways>These findings have been taken forward in a number of ways. The work on articulatory speech synthesis, supported by a Royal Society of Edinburgh / National Science Foundation China grant, has been further developed by researchers at Edinburgh and at the University of Science and Technology China. 

Work on articulatory modelling has had impact on speech therapy, through collaboration with Queen Margaret University, in part supported by the EPSRC project Ultrax.

Work on statistical speech synthesis, has resulted in a new area - voice banking - which has enabled the construction of natural-sounding, personalised synthetic voices from recordings of speech from people with disordered speech due to conditions such as Parkinson's disease or motor neurone disease. These synthetic voices are used in assistive technology devices that allow sufferers of these conditions to communicate more easily and effectively.</gtr:exploitationPathways><gtr:id>1DE747A3-BA66-46BB-BC79-B800346A30D3</gtr:id><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>http://www.mngu0.org</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Festival offers a general framework for building speech synthesis systems as well as including examples of various modules. As a whole it offers full text to speech through a number APIs: from shell level, though a Scheme command interpreter, as a C++ library, from Java, and an Emacs interface. Festival is multi-lingual (currently English (British and American), and Spanish) though English is the most advanced. Other groups release new languages for the system. And full tools and documentation for build new voices are available through Carnegie Mellon's FestVox project (http://festvox.org).

The software was first released in the 1990s, but has been under continuous development, improvement, and maintenance since then. v2.1 q
was released in November 2010.</gtr:description><gtr:id>53B8A338-1046-4B48-9FA6-0F9EAAB866BE</gtr:id><gtr:impact>Festival is distributed as default in a number of standard Linux distributions including Arch Linux, Fedora, CentOS, RHEL, Scientific Linux, Debian, Ubuntu, openSUSE, Mandriva, Mageia and Slackware, and can easily be installed on any Linux distribution that supports apt-get. More recently our work on statistical parametric speech synthesis and the algorithms for adaptation have been incorporated in the HTS toolkit (one of the coordinators (Yamagishi) is from Edinburgh), which integrates with Festival. These toolkits are the most used open-source speech synthesis systems and have also formed the high performing baseline systems for the international Blizzard evaluation of (commercial and research) speech synthesis also organised by Edinburgh.</gtr:impact><gtr:openSourceLicense>true</gtr:openSourceLicense><gtr:title>The Festival Speech Synthesis System</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.cstr.ed.ac.uk/projects/festival/</gtr:url></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/894BA521-FB2A-4EAC-A79F-DE8C04D4DBB1"><gtr:id>894BA521-FB2A-4EAC-A79F-DE8C04D4DBB1</gtr:id><gtr:title>HMM-based speech synthesiser using the LF-model of the glottal source</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/0354e6b9c5de838fe3e29ef2e8d29910"><gtr:id>0354e6b9c5de838fe3e29ef2e8d29910</gtr:id><gtr:otherNames>Cabral J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-0538-0</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/F6298EEF-8EEA-47E4-A590-05AC8D131529"><gtr:id>F6298EEF-8EEA-47E4-A590-05AC8D131529</gtr:id><gtr:title>Integrating Articulatory Features Into HMM-Based Parametric Speech Synthesis</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/2a43a7c784b9eb3272073c4b1094c50f"><gtr:id>2a43a7c784b9eb3272073c4b1094c50f</gtr:id><gtr:otherNames>Ling Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/66ABB214-D5B2-4CFD-9FE2-8E596B26B7A4"><gtr:id>66ABB214-D5B2-4CFD-9FE2-8E596B26B7A4</gtr:id><gtr:title>Glottal spectral separation for parametric speech synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/63a3fec0acf80e71b7aca23da3f6d5d4"><gtr:id>63a3fec0acf80e71b7aca23da3f6d5d4</gtr:id><gtr:otherNames>J Cabral</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/1631E138-D0D5-4EDA-89EF-FC840BE8F2CA"><gtr:id>1631E138-D0D5-4EDA-89EF-FC840BE8F2CA</gtr:id><gtr:title>Announcing the Electromagnetic Articulography (Day 1) Subset of the mngu0 Articulatory Corpus</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/8178e89faf76c9dcc8332494208275ee"><gtr:id>8178e89faf76c9dcc8332494208275ee</gtr:id><gtr:otherNames>Steve Renals (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/396829C6-FF4E-4AF4-B0C5-BC566173ED46"><gtr:id>396829C6-FF4E-4AF4-B0C5-BC566173ED46</gtr:id><gtr:title>Predicting tongue shapes from a few landmark locations</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/8e4e57dc85934f86576d2bffba23396e"><gtr:id>8e4e57dc85934f86576d2bffba23396e</gtr:id><gtr:otherNames>C Qin</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/4EEAC888-EFA2-4CC6-84CE-444435960A9B"><gtr:id>4EEAC888-EFA2-4CC6-84CE-444435960A9B</gtr:id><gtr:title>Preliminary inversion mapping results with a new EMA corpus</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/9180ee3ce26c6e141bda686fb6e1d338"><gtr:id>9180ee3ce26c6e141bda686fb6e1d338</gtr:id><gtr:otherNames>K Richmond</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/BDAEC480-3566-4DF1-8D79-605BC646C2EF"><gtr:id>BDAEC480-3566-4DF1-8D79-605BC646C2EF</gtr:id><gtr:title>The magnetic resonance imaging subset of the mngu0 articulatory corpus.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/0b2c8bbbbaad9b5cc83eb4f98830f2e6"><gtr:id>0b2c8bbbbaad9b5cc83eb4f98830f2e6</gtr:id><gtr:otherNames>Steiner I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/0685F7A8-E4CB-439A-99B4-C07BCB82808A"><gtr:id>0685F7A8-E4CB-439A-99B4-C07BCB82808A</gtr:id><gtr:title>Articulatory control of HMM-based parametric speech synthesis driven by phonetic knowledge</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/9180ee3ce26c6e141bda686fb6e1d338"><gtr:id>9180ee3ce26c6e141bda686fb6e1d338</gtr:id><gtr:otherNames>K Richmond</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E027741/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>B94A2498-60DA-4055-A957-686B6CB42654</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Linguistics</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>15BC6F17-6453-42B4-836A-01286E6D8068</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Comput./Corpus Linguistics</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>