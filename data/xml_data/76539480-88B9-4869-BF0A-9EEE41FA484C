<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/08E9C038-DB22-4F79-B1D8-F8228F8DF0E4"><gtr:id>08E9C038-DB22-4F79-B1D8-F8228F8DF0E4</gtr:id><gtr:firstName>Oded</gtr:firstName><gtr:surname>Ben-Tal</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/F2F2E7CB-2E6B-4611-9DB6-985CD82F7C75"><gtr:id>F2F2E7CB-2E6B-4611-9DB6-985CD82F7C75</gtr:id><gtr:firstName>Bob</gtr:firstName><gtr:surname>Sturm</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=AH%2FR004706%2F1"><gtr:id>76539480-88B9-4869-BF0A-9EEE41FA484C</gtr:id><gtr:title>Engaging three user communities with applications and outcomes of computational music creativity</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>AH/R004706/1</gtr:grantReference><gtr:abstractText>In recent years, machine learning and artificial intelligence systems have been grabbing headlines by defeating humans at the game of Go and the American TV game-show Jeopardy! The same technology have also been applied to other domains considered essentially human, like art and music. This project proposes an alternative viewpoint to the human-vs-machine paradigm: these technologies can produce co-creative partners that are useful, meaningful, and non-threatening.

This project facilitates four innovative and creative activities to engage with three user communities: Composers, Performers, and Listeners. The principal aim of this project is to engage these user communities with applications and outcomes of computational music creativity. A subsidiary aim is for the investigators to collaboratively learn from these communities about the implications of emergent computational music creativity in non-academic contexts. 

Among its objectives, this project will develop a software application to enable anyone with an Internet connection to explore music co-creation. The project will also develop an online resource for the user community of this application and others, where users can share, comment and rate submitted outcomes. The technology underpinning this application comes from the AHRC-funded Care for the Future research project, &amp;quot;Data science for the study of calypso-rhythm through history&amp;quot; (DaCaRyH, AH/N504531/1). That project brings together ethnomusicologists, experts in data science and a composer to investigate how computational tools can be used to study calypso music, and how the study of music can be used to inform the design of computational analyses. An unanticipated outcome of DaCaRyH is that some of these analytical tools can be &amp;quot;inverted&amp;quot; to create systems that synthesise never-before-heard music that is surprisingly good. Preliminary work by the investigators (in the form of expert elicitations, a public workshop, and music concert) show there is great interest among listeners and practitioners for exploring computational creativity outside of the academic realm.

The project will engage public audiences with music arising from computational co-creativity. It will deliver a hands-on workshop for composers to learn to use the developed software application. It will also organise a symposium on computational music creativity for the general public. The conclusion of the project will feature a music concert showcasing new music created with the developed application. To motivate participation, the project will organise a competition to solicit original compositions created with the application, to be performed by a professional ensemble. The competition jury will include musicians, composers, and of course a critic built using artificial intelligence. 

An especially novel outcome of this project is a professionally produced album of &amp;quot;machine folk&amp;quot; music. The album will feature music generated entirely by the system and edited, arranged, and performed by experienced musicians. The preliminary work of the investigators has found that good music performed by good musicians serves as the best ambassador for the value and potential usefulness of computational creativity. This album will serve as a lasting illustration for the benefits of the co-creative approach to machine learning tools.

This project links directly with the Digital Transformations theme by bringing state of the art digital technology to democratise participation in creative music making. It also relates to the Care for the Future theme by encouraging innovation and renewal in community music practice, and thus helping to broaden and deepen engagement from listeners, performers and composers. The project will serve to diversify the legacy of the DaCaRyH project.</gtr:abstractText><gtr:potentialImpactText>This project will impact users and beneficiaries outside the academic research community in three sectors.

# Public and third sector
- Professional music composers: One commercial composer with whom the investigators are collaborating has envisioned that the application to be developed in this project could help him overcome &amp;quot;the intimidation of the blank page&amp;quot;. Were he to begin by selecting among a wealth of materials, the process of composition would flow. Both investigators have composed new music using the current music models, and can see great benefit to this approach. The application will facilitate more of this kind of interaction.
- Professional music performers: Several performers the investigators have worked with so far already benefited from this work. One performer remarked that the process of preparing to perform tunes generated from the system led him to reevaluate his relationship with technology and his ideas about creative practice. Another performer said they didn't initially like the idea of engaging with a computer composing music, but once he heard the results he changed his mind.
- Education: The application to be developed in this project could be used as a pedagogical assistant to music students. In fact, part of the testing of the prototype application in this project will use music students at Kingston University. This classroom setting will be useful to test the usability of the application itself, but also the potential for this approach for teaching students about music and creativity.

# Commercial private sector
- New co-creative music enterprises: the topic of machine learning for music applications is attracting the interest of many commercial enterprises (e.g., London-based JukeDeck, Sony's Flow machines, Google's Magenta project). The application to be designed in this project (as well as the proof of the quality of the music in the form of the album) will motivate the development of similar co-creative music products.
- Standardised music testing industry: In the UK, there is a large cottage industry of preparatory services for national examinations in music. The music models developed in this project could be used to assist in the evaluation of the answers, such as stylistic relevance and originality. These will enhance the validity of such exams and also make the marking more efficient.

# Wider Public
- Amateur and semi-professional musicians: Non-professional music making is widespread in the UK with amateur choirs, orchestras, ensembles, and bands assembling to enjoy music making. This project will engage people who want music to be part of their lives, and offer them new opportunities for active and creative engagement with music through the application of new technology. In fact, one of the participants at the recent workshop of the investigators (http://www.insideoutfestival.org.uk/2017/events/folk-music-composed-by-a-computer/) commented that she felt unable to channel her creativity into making music because she lacks training. With the application to be developed in this project, she could start by generating tunes and selecting the bits she likes, building as she goes. The benefit, therefore, is both in the form of a tangible musical output but also in the form of learning and developing of musical skills.
- Public at large: Public interest in the subject of the project is evidenced by the number of views of our article for The Conversation (https://theconversation.com/machine-folk-music-shows-the-creative-side-of-ai-74708) and the examples posted to The Bottomless Tune Box YouTube channel (https://www.youtube.com/channel/UC7wzmG64y2IbTUeWji_qKhA). This indicates that there is a non-academic audience interested in engaging with the larger question of this project: how can artificial intelligence have a beneficial impact on music?</gtr:potentialImpactText><gtr:fund><gtr:end>2018-05-31</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/1291772D-DFCE-493A-AEE7-24F7EEAFE0E9"><gtr:id>1291772D-DFCE-493A-AEE7-24F7EEAFE0E9</gtr:id><gtr:name>AHRC</gtr:name></gtr:funder><gtr:start>2017-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>70990</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">AH/R004706/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>0AEFDABE-67A4-48B1-9DB4-99393BDE6065</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Music</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>A13304AD-8058-4333-86C7-DB798216A696</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Composition</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>55773495-BB0B-43EB-B99D-D5C15272A52F</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Musical Performance</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>D417837F-1924-4F1E-875B-A160B88B4C98</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Traditional Music</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>