<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/4E8EB4C1-E188-4F55-BAD5-C8CC9793F508"><gtr:id>4E8EB4C1-E188-4F55-BAD5-C8CC9793F508</gtr:id><gtr:firstName>Alan</gtr:firstName><gtr:surname>Chalmers</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FD036712%2F1"><gtr:id>A7A14883-DE57-45B1-8B95-ECF2E7A62B7A</gtr:id><gtr:title>Case for Support: A Feasability Study into Visual Attention Models for Exploring Complex Information.</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/D036712/1</gtr:grantReference><gtr:abstractText>This proposal presents a feasibility study into the potential for models of visual attention to enhance usable access by visually disabled users to complex structured information such as graph structures and electronic content presented on the web. Visually disabled people often use screenreaders that provide spoken output to access a page's content. Screenreaders, by default, simply present content from top-left to bottom-right. Given the complex nature of most graph structures and web pages, such a default reading is highly inappropriate. Screenreaders also allow their users to direct what is to be read by use of the cursor keys, yet without an overview of the page it is unclear how the focus of reading could be intuitively directed.Sighted readers, however, are able to direct their attention to the area of electronic content they are most interested in exploring. This is accomplished by obtaining a quick overview of the data presented through a glance. Models of visual attention can both describe and predict those areas of the screen which are most perceptually important. Factors such as colour and contrast information will draw the attention of observers. This coupled with task-based semantic knowledge of what to ignore and what to consider, provides a sighted reader with a quick starting point to choose how to explore structured data.Such models of visual attention have been used to selectively direct portions of virtual environments that should be rendered in high quality and to endow avatars with seemingly human like gesture behaviour. We wish to determine whether these may be adapted to use in the novel application context of re-presenting complex graph structures and web pages to visually disabled readers? We will explore the feasibility of this hypothesis by carrying out two specific case studies. The first being an exploration of a 3D molecular structure and the second a re-presentation of a class of web pages for content and media web sites (such as news sites). The first of the two case studies will be evaluated in terms of task completion times and subjective results from questionnaires assessing the effectiveness of non-visual exploration of the data presented. The second study will be evaluated using two methods; first through a comparison with an alternative method for re-presentation of web pages based on existing semantic web research at Manchester, and second through a comparison with eye-tracked data from sighted participants at Bristol. If these initial experiments prove successful, a full proposal for a collaborative research project to design an implement a suite of models tailored to non-visual presentation of complex information will be submitted to the UK-EPSRC.</gtr:abstractText><gtr:fund><gtr:end>2006-11-30</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-02-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>13516</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/462F3288-76EC-4AC4-BF8A-702C62EB119D"><gtr:id>462F3288-76EC-4AC4-BF8A-702C62EB119D</gtr:id><gtr:title>How people use presentation to search for a link: Expanding the understanding of accessibility on the web</gtr:title><gtr:parentPublicationTitle>Universal Access in the Information Society Journal</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/d586a63903f7ded9afda697b1d3f5913"><gtr:id>d586a63903f7ded9afda697b1d3f5913</gtr:id><gtr:otherNames>C Jay</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2006-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/81EFE03F-6EAE-41D2-9D0A-74704F794A7C"><gtr:id>81EFE03F-6EAE-41D2-9D0A-74704F794A7C</gtr:id><gtr:title>How people use presentation to search for a link: Expanding the understanding of accessibility on the web</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/d586a63903f7ded9afda697b1d3f5913"><gtr:id>d586a63903f7ded9afda697b1d3f5913</gtr:id><gtr:otherNames>C Jay</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2006-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/D036712/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>