<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/EC23DA53-CA73-4104-A3F6-2A9523484E69"><gtr:id>EC23DA53-CA73-4104-A3F6-2A9523484E69</gtr:id><gtr:name>Queen's University of Belfast</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>University Road</gtr:line1><gtr:line4>Belfast</gtr:line4><gtr:line5>County Antrim</gtr:line5><gtr:postCode>BT7 1NN</gtr:postCode><gtr:region>Northern Ireland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/EC23DA53-CA73-4104-A3F6-2A9523484E69"><gtr:id>EC23DA53-CA73-4104-A3F6-2A9523484E69</gtr:id><gtr:name>Queen's University of Belfast</gtr:name><gtr:address><gtr:line1>University Road</gtr:line1><gtr:line4>Belfast</gtr:line4><gtr:line5>County Antrim</gtr:line5><gtr:postCode>BT7 1NN</gtr:postCode><gtr:region>Northern Ireland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/085489EA-698D-4760-AC23-4257CD4B0C93"><gtr:id>085489EA-698D-4760-AC23-4257CD4B0C93</gtr:id><gtr:firstName>Danny</gtr:firstName><gtr:surname>Crookes</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/2BA536B4-D78D-4E94-B962-FF855E258EF2"><gtr:id>2BA536B4-D78D-4E94-B962-FF855E258EF2</gtr:id><gtr:firstName>Ming</gtr:firstName><gtr:surname>Ji</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FG001960%2F1"><gtr:id>BB291C36-F31F-4702-8FBC-603EE3ACF228</gtr:id><gtr:title>Corpus-Based Speech Separation</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/G001960/1</gtr:grantReference><gtr:abstractText>In this project, we will develop new techniques for restoring clear speech from noisy recordings. We will focus on two problems: (1) retrieving speech from background noise, and (2) separating speech sentences spoken by different speakers. For convenience, we reference both problems as speech separation.Over the past decades, there have been many techniques developed for speech separation. While appearing in different forms, most techniques can be viewed as a filter, which aims to pass the frequencies of the targeted speech with minimum distortion, and at the same time block the frequencies of the noise. To build the filter, one thus needs knowledge about the frequency structure of the noise. For certain applications in which the noise remains relatively constant, one may obtain an estimate of the noise structure using the data observed at a time without speech, and then use it to predict the noise structure in the data containing mixed speech and noise. Based on the prediction, a filter can be formed to remove the noise and hence restore the speech. Unfortunately, this strategy does not work if the noise changes fast and thus is unpredictable. Examples of fast-varying noises include crosstalk speech, and the background noises in mobile/Internet communications, which are often complex, highly dynamic, and thus difficult to predict.In this research, we will investigate a new method to speech separation, aiming for the capability of handling unpredictable noise. We will use a pre-recorded speech corpus, consisting of clean speech sentences by various speakers, to help remove the requirement for information about the noise. The new method consists of four major components. First, we compare the noisy sentence, containing mixed speech and noise, with each corpus sentence to find all their matching parts. Second, we combine the longest matching parts from the clean corpus sentences to form a new sentence, as a reconstruction of the target speech. Because of the richer and more distinct contexts, longer speech utterances are less confused by noise, and thus can be recognised with fewer errors than shorter utterances. This explains why we synthesise the target speech using longest recognised speech parts, which minimises the effect of noise on the restoration. The third component of our method is a novel technique to reduce the sensitivity to noise for finding the matching speech parts between the noisy and corpus sentences. The last component uses the speakers characteristics, associated with the individual corpus sentences, to help separate mixed sentences spoken by different speakers. Combining these components, the new method offers the capability to separate speech from noise, and separate mixed speech sentences, without having to predict the noise/crosstalk.</gtr:abstractText><gtr:fund><gtr:end>2011-09-30</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>312394</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Research Impact Showcase</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>CB56B204-5E14-4B72-83BC-993B98A017FD</gtr:id><gtr:impact>Our work on speech enhancement and separation (funded by EPSRC and EPSRC KTS, in collaborations with CSR) was selected to be exhibited in the Research Impact Showcase at Queen's on 27 November 2013. The work was published in 'The DNA of Innovation, Volume 3: Creative Connections', interviewed by BBC Radio Ulster News, and was also published on the Belfast Telegraph.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2013</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>38671</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC Knowledge Transfer Secondments (KTS)</gtr:description><gtr:end>2012-09-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>KTS-1117 (Queen's University Belfast)</gtr:fundingRef><gtr:id>41B41F85-9DB7-45A6-9D4B-D4257A059A4F</gtr:id><gtr:sector>Academic/University</gtr:sector><gtr:start>2011-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our research findings from this research have led to impact on a range of different fronts, mainly:
(i) Collaboration with CSR, a leading $1 billion consumer electronics company, has shaped its R/D research agenda in speech enhancement, has inspired ideas for new product improvements, and has helped establish Belfast as an audio research centre of excellence within the company.

(ii) Collaboration with Vitalograph Ltd, a company delivering healthcare monitoring systems, has led to a robust, acoustic-based monitoring system for medical inhalers, with potential for multi-million pound savings in NHS budgets.

(iii) NTT (Nippon Telegraph &amp;amp; Telephone Corp.) used our method in their speech recognition entry to the International Competition for Machine Listening in Multisource Environments (CHiME 2011), in which they took 1st place.

(iv) In 2014, Microsoft has used our technique developed from this project in their system for noisy speech bandwidth expansion, for details, see H. Seo, H. Kang, and F. Soong, &amp;quot;A maximum a posterior-based reconstruction approach to speech bandwidth expansion in noise,&amp;quot; ICASSP 2014, pp. 6128-6132.</gtr:description><gtr:id>306B4CB8-FB0A-43DC-8E90-42DA6C2A65AA</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>In the real world, speech rarely occurs in isolation, and is usually accompanied by other acoustic interference. The two most common scenarios are: (1) speech is accompanied by some background noise, e.g., cocktail party noise, background music, street noise or any other environmental noise, and (2) one speaker's voice is masked by other speakers' voices, which happens when two or more people speak simultaneously. Severe interference can make speech unintelligible. Restoring clear speech from noise and separating crosstalk voices are two major and unsolved problems in signal processing research. The problems become extremely difficult if the noise is fast-varying and hence potentially unpredictable, and if the crosstalk voices are arbitrary in language, vocabulary and structure. These problems are further compounded when there is only a single microphone to record the noisy or mixed voices (the 'single channel' problem).

This project has developed a radically different and effective solution to the above problems, i.e., a corpus-based approach to single-channel speech enhancement and separation. The new method uses speech corpora as examples, combined with novel signal modelling techniques, to provide an accurate model for speech, both for what it sounds like (the human aspect) and for how it evolves over time (the language aspect). The new model of speech can reach a level of accuracy (or sharpness) which is previously unattainable with existing techniques. This proves to be extremely effective in extracting speech buried in noise and separating simultaneous voices from different speakers; and it does this with effectively no limitations on the complexity of the noise and the language. Large numbers of experiments have demonstrated that the new method can significantly outperform existing techniques for dealing with fast-varying noise and arbitrary crosstalk, and hence has raised the state-of-the-art performance to a new level.</gtr:description><gtr:exploitationPathways>(1) NTT (Nippon Telegraph &amp;amp; Telephone Corp.) used our method in their speech recognition entry to the International Competition for Machine Listening in Multisource Environments (CHiME 2011), in which they took 1st place. NTT has further exploited our method to handle speech de-reverberation (Interspeech'2011).

(2) CSR has rated 5 out of 5 for the significance of the outcomes of the joint KTS project to their organisations future performance. (1) A patent was filed in 08/2012 (International Application No. PCT/EP2012/066549). 

(3) A collaboration agreement was signed between QUB and Cambridge Silicon Radio for a potential commercial development of the new technology for in-car and wireless communication applications. The researcher on the grant is undertaking a secondment to CSR, supported by a grant from the follow-on EPSRC KTS scheme, and by the technology transfer facilities available in the QUB ECIT institute (2011-2012).

(4) Collaboration with Vitalograph Ltd, a company delivering healthcare monitoring systems, has led to a robust, acoustic-based monitoring system for medical inhalers, with potential for multi-million pound savings in NHS budgets. This project was awarded the 2013 InterTradeIreland Fusion Project Exemplar award. Further on-going collaborations with the company include the development of an automatic cough detection system based on our robust speeh processing techniques arising from the EPSRC project.

(5) In 2014, our techniques developed from this project was used by Microsoft in their system for speech bandwidth expansion with noise (ICASSP 2014).</gtr:exploitationPathways><gtr:id>91A5299B-F1CD-488B-AC5A-9D1A5E6FB9DD</gtr:id><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>http://www.ecit.qub.ac.uk/Research/SpeechVisionSystems/SpeechSeparation/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/8051076A-B695-4248-A44A-2F60DC53C331"><gtr:id>8051076A-B695-4248-A44A-2F60DC53C331</gtr:id><gtr:title>A longest matching segment approach with Baysian adaptation - application to noise-robust speaker recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/d5191de8a3f85fb044d1ddceb841c156"><gtr:id>d5191de8a3f85fb044d1ddceb841c156</gtr:id><gtr:otherNames>Ayeh Jafari (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/A122C17C-981E-461F-97B7-E4F37AB1EAC2"><gtr:id>A122C17C-981E-461F-97B7-E4F37AB1EAC2</gtr:id><gtr:title>CLOSE-A Data-Driven Approach to Speech Separation</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/5cd292dc1cdc2ef6bc08f147f3fabc65"><gtr:id>5cd292dc1cdc2ef6bc08f147f3fabc65</gtr:id><gtr:otherNames>Ji Ming</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/C8D19726-CA9A-4BA5-A01B-1E6147F1E2D4"><gtr:id>C8D19726-CA9A-4BA5-A01B-1E6147F1E2D4</gtr:id><gtr:title>Maximizing the continuity in segmentation - A new approach to model, segment and recognize speech</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/5cd292dc1cdc2ef6bc08f147f3fabc65"><gtr:id>5cd292dc1cdc2ef6bc08f147f3fabc65</gtr:id><gtr:otherNames>Ji Ming</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-2353-8</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/B3FAFEE7-FECA-4C73-B80F-C52C60EC9C2C"><gtr:id>B3FAFEE7-FECA-4C73-B80F-C52C60EC9C2C</gtr:id><gtr:title>A corpus-based approach to speech enhancement from nonstationary noise</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/e9974ea1493f2af6d1bcfcc7559ad2e9"><gtr:id>e9974ea1493f2af6d1bcfcc7559ad2e9</gtr:id><gtr:otherNames>Ji Ming (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/163E6ED8-F407-473D-8AB5-265889407AA9"><gtr:id>163E6ED8-F407-473D-8AB5-265889407AA9</gtr:id><gtr:title>Single-channel speaker-pair identification: A new approach based on automatic frame selection</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/7d665eeb541ee7870a0c405ab2dbaeb9"><gtr:id>7d665eeb541ee7870a0c405ab2dbaeb9</gtr:id><gtr:otherNames>Srinivasan R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/BE26AB57-DD86-4B68-99B0-9BD1108848BF"><gtr:id>BE26AB57-DD86-4B68-99B0-9BD1108848BF</gtr:id><gtr:title>Robust audio-visual speech recognition under noisy audio-video conditions.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on cybernetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/8e0c35b307135b4a964c057f6385284d"><gtr:id>8e0c35b307135b4a964c057f6385284d</gtr:id><gtr:otherNames>Stewart D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>2168-2267</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/021543E5-AB79-4AE5-A638-850136D1D27C"><gtr:id>021543E5-AB79-4AE5-A638-850136D1D27C</gtr:id><gtr:title>Speech Enhancement from Additive Noise and Channel Distortion - A Corpus-Based Approach</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/59a7a98517ad7ee4e6542c557ddd2a8b"><gtr:id>59a7a98517ad7ee4e6542c557ddd2a8b</gtr:id><gtr:otherNames>Ji M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/1C1AAF1B-7588-4DB4-984F-277CBF8A740F"><gtr:id>1C1AAF1B-7588-4DB4-984F-277CBF8A740F</gtr:id><gtr:title>A Corpus-Based Approach to Speech Enhancement From Nonstationary Noise</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/5cd292dc1cdc2ef6bc08f147f3fabc65"><gtr:id>5cd292dc1cdc2ef6bc08f147f3fabc65</gtr:id><gtr:otherNames>Ji Ming</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/0CFB5D19-162B-4235-9228-A516347955FA"><gtr:id>0CFB5D19-162B-4235-9228-A516347955FA</gtr:id><gtr:title>An iterative longest matching segment approach to speech enhancement with additive noise and channel distortion</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/a231d23241e31b7cac2ab9936ed8e0c5"><gtr:id>a231d23241e31b7cac2ab9936ed8e0c5</gtr:id><gtr:otherNames>Ming J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/8037D424-5EAF-4CD5-82E6-153D1720CDDA"><gtr:id>8037D424-5EAF-4CD5-82E6-153D1720CDDA</gtr:id><gtr:title>A longest matching segment approach for text-independent speaker recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/d5191de8a3f85fb044d1ddceb841c156"><gtr:id>d5191de8a3f85fb044d1ddceb841c156</gtr:id><gtr:otherNames>Ayeh Jafari (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/G001960/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>B94A2498-60DA-4055-A957-686B6CB42654</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Linguistics</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>15BC6F17-6453-42B4-836A-01286E6D8068</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Comput./Corpus Linguistics</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>