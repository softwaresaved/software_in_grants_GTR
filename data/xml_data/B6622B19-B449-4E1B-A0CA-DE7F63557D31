<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/BA42FE9E-1091-4F26-9645-58A2BECFD230"><gtr:id>BA42FE9E-1091-4F26-9645-58A2BECFD230</gtr:id><gtr:name>University of Bedfordshire</gtr:name><gtr:department>Inst for Res in Applicable Computing</gtr:department><gtr:address><gtr:line1>Park Square</gtr:line1><gtr:line4>Luton</gtr:line4><gtr:line5>Bedfordshire</gtr:line5><gtr:postCode>LU1 3JU</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/BA42FE9E-1091-4F26-9645-58A2BECFD230"><gtr:id>BA42FE9E-1091-4F26-9645-58A2BECFD230</gtr:id><gtr:name>University of Bedfordshire</gtr:name><gtr:address><gtr:line1>Park Square</gtr:line1><gtr:line4>Luton</gtr:line4><gtr:line5>Bedfordshire</gtr:line5><gtr:postCode>LU1 3JU</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/9C72232E-859D-46DE-A25C-65F69B52DCBA"><gtr:id>9C72232E-859D-46DE-A25C-65F69B52DCBA</gtr:id><gtr:firstName>Feng</gtr:firstName><gtr:surname>Dong</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/67F57B8C-202A-4941-BC5F-296D8461BE0B"><gtr:id>67F57B8C-202A-4941-BC5F-296D8461BE0B</gtr:id><gtr:firstName>Gordon</gtr:firstName><gtr:surname>Clapworthy</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FF066473%2F1"><gtr:id>B6622B19-B449-4E1B-A0CA-DE7F63557D31</gtr:id><gtr:title>Animating Humans from Static Images via an Entirely Image-Based Approach</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/F066473/1</gtr:grantReference><gtr:abstractText>Images/videos have a promising future for figure animation, an entirely image/video-based approach would allow us to achieve high realism by directly utilising real images/videos. Unfortunately, making effective use of real world images/videos is not a simple task and it is very difficult to reconstruct 3D arbitrary views for human motion. Currently, the Image/Video Based Rendering(IVBR) achieves this by using either captured or adapted generic human geometry from 3D scanners or multi-cameras, which involves costly resources.In fact, the human brain has a strong in-built capacity to imagine motion from static objects. Given a few images of a human motion, we can easily interpret them by envisaging a virtual movement in our mind, without the need of any geometric information. However, existing computing technology still largely falls short of such a capability. Motivated by this observation, the proposed research is designed to take a highly speculative adventure which will explore novel techniques to equip computers with such an ability. Generally speaking, we shall look into the feasibility of making human characters alive from their static images, allowing arbitrary views of their movement to be directly reconstructed from a few key images without requiring their geometric models. While we target humans here, the methodology examined can be applicable to a broad range of articulated/non-articulated subjects. It will go beyond the current form of IVBR, which was mainly designed for objects with fixed shapes, and will aim to achieve what is traditionally feasible only with the assistance of geometric models. It could lead to an alternative that is fundamentally different from all current techniques.This feasibility study will concentrate only on the most fundamental issue of the entirely image based approach , which is the View Reconstruction for Humans (VRH), i.e. whether we can create images of a human movement under arbitrary viewpoints just from a few static images. To test the idea without losing generality, many datasets involved in our experiment will be created by computers. Once a solution to VRH is found, it will open the door to further investigation using real captured images for the training and also to work on other important issues concerning control, data organization &amp;amp; compression, image compositions, hair and cloth motion, etc., in follow-on projects. To allow for the completion of this feasibility study in a short period, we have designed a detailed research route. A learning-based approach will be taken to build statistical models for image sequences of human motion through training from existing examples. Subsequently, such models will be used to construct new sequences of human motion. While there are many potential ways to provide effective user controls, in order to stay focused on VRH, we shall take the most straightforward control strategy, which will use a selected number of images to indicate the key postures of the actor over the time. This is analogous to the key-frame control strategy that is widely adopted in animation.This research also has strong commercial potential in a broad range of entertainment-related businesses in areas such as image/video editing, computer games, the film industry, etc. They have a major presence in the UK and generate significant global income. It will be actively invovled by our industrial contacts at Antics Technologies and Cinesite. Cinesite is one of the largest companies in the production of computer visual effects and post production in the world, while Antics Technologies provides revolutionary software for full computer animation and has world-wide customers. They have recognized the potential market values of this research and will provide strong support through consultancy, evaluation and exploitation. Antics will provide their latest animation software release for this research at no cost.</gtr:abstractText><gtr:fund><gtr:end>2009-03-31</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>80081</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/B86F9B36-5002-4958-AEEE-1C225EC101F6"><gtr:id>B86F9B36-5002-4958-AEEE-1C225EC101F6</gtr:id><gtr:title>Manifold-constrained coding and sparse representation for human action recognition</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/f0d10ae0b5fee7f7d7a790e1b961f1e4"><gtr:id>f0d10ae0b5fee7f7d7a790e1b961f1e4</gtr:id><gtr:otherNames>Zhang X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/A3D73142-DC7A-498F-B8F3-1F7BD5BC5A20"><gtr:id>A3D73142-DC7A-498F-B8F3-1F7BD5BC5A20</gtr:id><gtr:title>Laplacian group sparse modeling of human actions</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/f0d10ae0b5fee7f7d7a790e1b961f1e4"><gtr:id>f0d10ae0b5fee7f7d7a790e1b961f1e4</gtr:id><gtr:otherNames>Zhang X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/D486C698-B48B-4706-AE0E-E8D01706212D"><gtr:id>D486C698-B48B-4706-AE0E-E8D01706212D</gtr:id><gtr:title>Contour synthesis by least-squares construction</gtr:title><gtr:parentPublicationTitle>Electronics Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/959ee3730ae4f7a093e458dc29d6a59a"><gtr:id>959ee3730ae4f7a093e458dc29d6a59a</gtr:id><gtr:otherNames>Dong F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/114ED6AE-E89B-4339-A8EE-672A519152EC"><gtr:id>114ED6AE-E89B-4339-A8EE-672A519152EC</gtr:id><gtr:title>Rendering of novel views from photographs using inference in Markov random field</gtr:title><gtr:parentPublicationTitle>Electronics Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/959ee3730ae4f7a093e458dc29d6a59a"><gtr:id>959ee3730ae4f7a093e458dc29d6a59a</gtr:id><gtr:otherNames>Dong F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/F066473/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>4A856E02-8C94-4981-B572-E381EEB60DD2</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Media</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>972C8509-5001-4523-9E10-7FA67E2F2E69</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Visual arts</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>8BB67751-1AD5-4EE8-9104-089D5F236DDB</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Digital Arts HTP</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>2968C870-0FF4-4AE9-A670-9B0161C83B37</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>New Media/Web-Based Studies</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>