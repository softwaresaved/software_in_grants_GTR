<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/5BFB9036-9D16-4AB9-A9EF-097BB6FBD69A"><gtr:id>5BFB9036-9D16-4AB9-A9EF-097BB6FBD69A</gtr:id><gtr:name>Aston University</gtr:name><gtr:department>Sch of Life and Health Sciences</gtr:department><gtr:address><gtr:line1>Aston Triangle</gtr:line1><gtr:line4>Birmingham</gtr:line4><gtr:postCode>B4 7ET</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/5BFB9036-9D16-4AB9-A9EF-097BB6FBD69A"><gtr:id>5BFB9036-9D16-4AB9-A9EF-097BB6FBD69A</gtr:id><gtr:name>Aston University</gtr:name><gtr:address><gtr:line1>Aston Triangle</gtr:line1><gtr:line4>Birmingham</gtr:line4><gtr:postCode>B4 7ET</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/E2D70EF4-C897-47E8-BD73-19D9E50926AC"><gtr:id>E2D70EF4-C897-47E8-BD73-19D9E50926AC</gtr:id><gtr:name>Starkey Hearing Technologies</gtr:name><gtr:address><gtr:line1>6700 Washington Avenue S</gtr:line1><gtr:region>Unknown</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/A306E84B-3117-4BE1-9760-FA0E44EA3BB3"><gtr:id>A306E84B-3117-4BE1-9760-FA0E44EA3BB3</gtr:id><gtr:name>Audience Inc</gtr:name><gtr:address><gtr:line1>331 Fairchild Drive</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/3520E6DF-FD6B-4533-AB49-1F3AC1E1197B"><gtr:id>3520E6DF-FD6B-4533-AB49-1F3AC1E1197B</gtr:id><gtr:firstName>Brian</gtr:firstName><gtr:surname>Roberts</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/684D93E7-61D9-4FE1-8076-436E2CCF4987"><gtr:id>684D93E7-61D9-4FE1-8076-436E2CCF4987</gtr:id><gtr:firstName>Robert</gtr:firstName><gtr:otherNames>James</gtr:otherNames><gtr:surname>Summers</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=ES%2FN014383%2F1"><gtr:id>336217C6-1116-4C49-B633-55C10EF021C0</gtr:id><gtr:title>Interference in spoken communication: Evaluating the corrupting and disrupting effects of other voices</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/N014383/1</gtr:grantReference><gtr:abstractText>In everyday life, talking with other people is important not only for sharing knowledge and ideas, but also for maintaining a sense of belonging to a community. Most people take it for granted that they can converse with others with little or no effort. Successful communication involves understanding what is being said and being understood, but it is quite rare to hear the speech of a particular talker in isolation. Speech is typically heard in the presence of interfering sounds, which are often the voices of other talkers. The human auditory system, which is responsible for our sense of hearing, therefore faces the challenge of identifying which parts of the sounds reaching our ears have come from which talker.

Solving this &amp;quot;auditory scene analysis&amp;quot; problem involves separating those sound elements arising from one source (e.g., the voice of the talker to whom you are attending) from those arising from other sources, so that the identity and meaning of the target source can be interpreted by higher-level processes in the brain. Over the course of evolution, humans have been exposed to a variety of complex listening environments, and so we are generally very successful at understanding the speech of one person in the presence of other talkers. This contrasts with attempts to develop listening machines, which often fail when confronted with adverse conditions, such as automatic transcription of a conversation in an open-plan office. Human listeners with hearing impairment often find these environments especially difficult, even when using the latest developments in hearing-aid or cochlear-implant design, and so can struggle to communicate effectively in such conditions.

Much of the information necessary to understand speech (acoustic-phonetic information) is carried by the changes in frequency over time of a few broad peaks in the frequency spectrum of the speech signal, known as formants. The project aims to investigate how listeners presented with mixtures of target speech and interfering formants are able to group together the appropriate formants, and to reject others, such that the speech of the talker we want to listen to can be understood. Interfering sounds can have two kinds of effect - energetic masking, in which the neural response of the ear to the target is swamped by the response to the masker, and informational masking, in which the &amp;quot;auditory brain&amp;quot; fails to separate readily detectable parts of the target from the masker. The project will explore the informational masking component of interference - often the primary factor limiting speech intelligibility - using stimulus configurations that eliminate energetic masking. We will do so using perceptual experiments in which we measure how our ability to understand speech (e.g., the number of words reported correctly) changes under a variety of conditions.

The project will examine how acoustic-phonetic information is combined across formants. It will also explore how a speech-like interferer affects intelligibility, distinguishing the circumstances in which the interferer takes up some of the available perceptual processing capacity from those in which specific properties of the interferer intrude into the perception of the target speech. Our approach is to use artificial speech-like stimuli with precisely controlled properties, to mix target speech with carefully designed interferers that offer alternative grouping possibilities, and to measure how manipulating the properties of these interferers affects listeners' abilities to recognise the target speech in the mixture. The results will improve our understanding of how human listeners separate speech from interfering sounds and the constraints on that separation, helping to refine computational models of listening. Such refinements will in turn provide ways of improving the performance of devices such as hearing aids and automatic speech recognisers when they operate in adverse listening conditions.</gtr:abstractText><gtr:potentialImpactText>Private-sector companies who develop hearing aids and cochlear implants:

Effective separation of a target voice from interfering speech is one of the main problems facing designers of hearing aids and cochlear implant (CI) processors. The project will provide greater understanding of how human listeners combine acoustic-phonetic information across formants and improved characterisation of the informational component of speech-on-speech interference. This will inform the development of enhanced hearing prostheses, and may potentially improve speech coding in CI processors. One of our Project Partners, Starkey, invests in longer-term issues in hearing research and is the largest manufacturer of hearing aids in the USA. We have also established contacts with the two major suppliers of hearing prostheses in the UK (Oticon and Phonak). Outcomes will be communicated to hearing aid and CI manufacturers by sending technical reports of our findings, by visits to selected companies, and meetings with their representatives at conferences.

Private-sector companies who develop robust automatic speech recognition (ASR) devices and techniques for speech enhancement:

Robust performance in the presence of interferers remains a key problem for ASR. The project will provide perceptual data for researchers in computational auditory scene analysis (CASA) to develop further models and algorithms for separating speech from interfering sounds; we already have established contacts with CASA researchers. This offers the prospect of improved front-end processors for ASR and speech enhancement systems, which is likely to improve the performance of commercial systems. Our other Project Partner, Audience, is one of the leading commercial developers of speech enhancement technology. Our research findings will be communicated to Audience via email and at regular meetings. A report will also be made available to other key companies in ASR and enhancement technology.

Charities that support the hearing impaired:

Our research findings will be communicated to the main UK charity, Action on Hearing Loss. Improved understanding of the perceptual processes underpinning successful separation of target speech from interfering speech will encourage the development of collaborative behavioural and modelling studies. Such cross-disciplinary interaction may stimulate funding of research projects on advanced hearing aids and CI algorithms likely to interest the charity. As well as articles in specialist journals, our findings will be communicated via a report tailored to the charity's interests.

The general public:

Indirectly, the impact on the above beneficiaries will also benefit the general public within 5-10 years. Improvements to hearing aid technology will benefit the ~10 million deaf and hard-of-hearing people in the UK. Similarly, there are ~320,000 CI users worldwide who would benefit from better techniques for encoding noisy speech in CI processors. Improved ASR and speech enhancement, which may arise from our project via its impact on CASA solutions, will enhance quality of life through improved speech-based communication with machines, and better electronically mediated vocal communication between individuals. The outcomes of the project will be communicated directly to the public via a lay summary on our project website, and through public engagement events.

The research fellow:

The RF will receive training on public engagement for researchers to facilitate his contributions to public lectures, report writing, and company visits. His involvement in writing journal articles and developing grant applications will put him in an excellent position for fellowship applications. His involvement in writing technical reports and visits to leading hearing-technology companies will foster links with the commercial sector likely to enhance his prospects of future employment either in academic or industrial research.</gtr:potentialImpactText><gtr:fund><gtr:end>2019-08-31</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2016-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>329960</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">ES/N014383/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>48D25546-6ADF-479A-8877-478CCDB1DC1F</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Animal Science</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>B94A2498-60DA-4055-A957-686B6CB42654</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Linguistics</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>E1AC33C6-9927-41AC-B23B-2EED8F593588</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Experimental Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>78C47607-4818-4A9C-B510-D5D9A368C83F</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Phonetics</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>F439A20B-A9B0-4A68-B703-7F6AE7570E39</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Systems neuroscience</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>