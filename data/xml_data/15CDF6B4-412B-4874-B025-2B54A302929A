<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.rcuk.ac.uk:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.rcuk.ac.uk:80/organisation/AFE5C6FD-3500-44F6-B100-184B5F2FD0D7"><gtr:id>AFE5C6FD-3500-44F6-B100-184B5F2FD0D7</gtr:id><gtr:name>Microsoft Research</gtr:name><gtr:address><gtr:line1>One Microsoft Way</gtr:line1><gtr:line4>Redmond</gtr:line4><gtr:line5>Washington 98052</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/E89C3602-0FB4-4044-A918-58966B8A10B2"><gtr:id>E89C3602-0FB4-4044-A918-58966B8A10B2</gtr:id><gtr:name>University of Reading</gtr:name><gtr:department>Sch of Psychology and Clinical Lang Sci</gtr:department><gtr:address><gtr:line1>Whiteknights House</gtr:line1><gtr:line2>PO Box 217</gtr:line2><gtr:line4>Reading</gtr:line4><gtr:line5>Berkshire</gtr:line5><gtr:postCode>RG6 6AH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/E89C3602-0FB4-4044-A918-58966B8A10B2"><gtr:id>E89C3602-0FB4-4044-A918-58966B8A10B2</gtr:id><gtr:name>University of Reading</gtr:name><gtr:address><gtr:line1>Whiteknights House</gtr:line1><gtr:line2>PO Box 217</gtr:line2><gtr:line4>Reading</gtr:line4><gtr:line5>Berkshire</gtr:line5><gtr:postCode>RG6 6AH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/AFE5C6FD-3500-44F6-B100-184B5F2FD0D7"><gtr:id>AFE5C6FD-3500-44F6-B100-184B5F2FD0D7</gtr:id><gtr:name>Microsoft Research</gtr:name><gtr:address><gtr:line1>One Microsoft Way</gtr:line1><gtr:line4>Redmond</gtr:line4><gtr:line5>Washington 98052</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/ECA5EB38-3074-41F0-B81E-B1DCF13A912E"><gtr:id>ECA5EB38-3074-41F0-B81E-B1DCF13A912E</gtr:id><gtr:name>Renault France</gtr:name><gtr:address><gtr:line1>13-15, Quai Le Gallo</gtr:line1><gtr:line2>Boulogne-Billancourt Cedex</gtr:line2><gtr:line3>92109</gtr:line3><gtr:region>Outside UK</gtr:region><gtr:country>France</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/F262B9E0-E9D1-4BFF-B9BF-B4C81D12D8B9"><gtr:id>F262B9E0-E9D1-4BFF-B9BF-B4C81D12D8B9</gtr:id><gtr:name>Microsoft Research Ltd</gtr:name><gtr:address><gtr:line1>21 Station Road</gtr:line1><gtr:postCode>CB1 2FB</gtr:postCode><gtr:region>Unknown</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/2646F1BE-DBD2-411B-9D89-C143536D1D43"><gtr:id>2646F1BE-DBD2-411B-9D89-C143536D1D43</gtr:id><gtr:firstName>Andrew</gtr:firstName><gtr:surname>Glennerster</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FK011766%2F1"><gtr:id>15CDF6B4-412B-4874-B025-2B54A302929A</gtr:id><gtr:title>Testing view-based and 3D models of human navigation and spatial perception</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K011766/1</gtr:grantReference><gtr:abstractText>The way that animals use visual information to move around and interact with objects involves a highly complex interaction between visual processing, neural representation and motor control. Understanding the mechanisms involved is of interest not only to neuroscientists but also to engineers who must solve similar problems when designing control systems for autonomous mobile robots and other visually guided devices.

Traditionally, neuroscientists have assumed that the representation delivered by the visual system and used by the motor system is something like a 3D model of the outside world, even if the reconstruction is a distorted version of reality. Recently, evidence against such a hypothesis has been mounting and an alternative type of theory has emerged. 'View-based' models propose that the brain stores and organises a large number of sensory contexts for potential actions. Instead of storing the 3D coordinates of objects, the brain creates a visual representation of a scene using 2D image parameters, such as widths or angles, and information about the way that these change as the observer moves. This project examines the human representation of three-dimensional scenes to help distinguish between these two opposing hypotheses.

To do this, we will use immersive virtual reality with freely-moving observers to test the predictions of the 3D reconstruction and 'view-based' models. Head-tracked virtual reality allows us to control the scene the observer sees and to track their movements accurately. Certain spatial abilities have been taken as evidence that the observer must create a 3D reconstruction of the scene in the brain. For example, people are able to view a scene, remember where objects are, walk to a new location and then point back to one of the objects they had seen originally even if it is no longer visible (i.e. people can update the visual direction of objects as they move). However, this capacity does not necessarily require that the brain generate a 3D model of the scene and, as evidence, we will extend view-based models to include this pointing task and others like it. We will then test the predictions of both view-based and 3D reconstruction models against the performance of human participants carrying out the same tasks. 

As well as predicting the pattern of errors in simple navigation and pointing tasks, we will also measure the effect of two types of stimulus change. 3D reconstruction uses 'corresponding points' which are points in an image that arise, for example, from the same physical object (or part of an object) as a camera or person moves around it. Using a novel stimulus, we will keep all of these 'corresponding points' in a scene constant yet, at the same time, changing the scene so that the images alter radically when the observer moves. This manipulation should have a dramatic effect on a view-based scheme but no effect at all on any system based only on corresponding points.

Overall, we will have a tight coupling between experimental observations and quantitative predictions of performance under two types of model. This will allow us to determine which of the two models most accurately reflects human behaviour in a 3D environment. One potential outcome of the project is that view-based models will provide a convincing account of performance in tasks that have previously been considered to require 3D reconstruction, opening up the possibility that a wide range of tasks can be explained within a view-based framework.</gtr:abstractText><gtr:potentialImpactText>Our experiments aim to deliver a more accurate model of human spatial representation and navigation behaviour than exists at present. There are clear industrial applications for this type of knowledge in several distinct areas. We have two existing collaborations that allow us to have a direct impact. First, we have a long-standing relationship with Microsoft Research Cambridge, who fund a current PhD student with us (since October 2011). Andrew Fitzgibbon, who co-supervises the project, and others at Microsoft (John Winn, Antonio Criminisi) are interested in algorithms that use non-Cartesian, view-based representations for applications that have traditionally relied on 3D metric models.

Second, we have a collaboration with the car manufacturer, Renault. They perform much of their car-interior prototyping in virtual reality, and are keenly interested in perception data coming from our lab in order to determine which types of scene manipulation will have a noticeable perceptual effect and which will not. They also have a strong interest in the calibration methods and high-quality virtual reality that we have available in our laboratory. Renault will fund a new PhD student in our lab starting in 2012.

Our laboratory is involved in a range of out-reach activities, including open days for the public and for school children from the Sutton Trust. Dr Glennerster has advertised the work of the lab giving plenary and other talks at 3DTV conferences, where producers and technologists alike are interested in problems with the way that 3DTV and 3D cinema is interpreted and perceived. Dr Glennerster has given public lectures (e.g. Royal College of Surgeons) and our laboratory has engaged the public in a demonstration at the Royal Society of the mutations involved in the potassium channel affected in neonatal diabetes: children could fly through a model of their own channel as it opened and closed and was 'mended' by the drug that cured their condition.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-09-30</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-02-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>419878</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Oxford</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>Department of Engineering Sciences</gtr:department><gtr:description>Collaboration with Phil Torr's group in Robotics, University of Oxford</gtr:description><gtr:id>C1AECC7A-6F13-4E5D-87B2-B6D1EA88E2D0</gtr:id><gtr:impact>Multidisciplinary: neuroscience and computer vision/machine learning.</gtr:impact><gtr:partnerContribution>The Torr group will carry out the modelling described above.</gtr:partnerContribution><gtr:piContribution>We have begun a collaboration that will be extended as part of EPSRC grant EP/N019423/1. We will provide access to the Virtual Reality lab in Reading and psychophysical expertise. The aim is to compare human performance on navigation tasks with that of reinforcement learning techniques trained on games that require navigation to obtain rewards.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Microsoft Research</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>Microsoft Research Cambridge</gtr:department><gtr:description>Collaboration with Microsoft Research, Cambridge</gtr:description><gtr:id>BE11A83D-BF00-43FB-A48E-B7CF6E51FADA</gtr:id><gtr:impact>Yes, this is multi-disciplinary (computer vision and human psychophysics). There are conference outputs, a PhD thesis nearly completed and publications in preparation.</gtr:impact><gtr:partnerContribution>co-supervising PhD student, trips to Reading, Cambridge and plans to hold a conference July 1-3rd 2015 at Microsoft Research Cambridge.</gtr:partnerContribution><gtr:piContribution>co-supervising PhD student</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>cBBC coverage</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>24B571D6-FFCD-4417-B7B9-F30EEB8F70A1</gtr:id><gtr:impact>cBBC approached us to ask about virtual reality. Our lab comes up readily on searches for virtual reality. We participated in a program about the future of VR. It included VR from the hap tics group in Systems Engineering at Reading with whom we collaborate.

Journalists at the BBC say they will contact us again in relation to similar topics.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>http://bbc.in/1nJ4f1N</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Microsoft Research Cambridge conference</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>3BB765E2-22DD-4DBC-BE97-5D45923AABD6</gtr:id><gtr:impact>15 academics from USA, Europe and UK and members of Microsoft Research (MSR) Cambridge met at MSR to discuss 'view-based' approaches to spatial representation. This led to very fruitful exchange of ideas between the computer vision and neuroscience communities and should result in two publications.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://www.glennersterlab.com/MSRMeeting2015/index.html</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Journal of Neuroscience press release</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>5D00154F-6632-43E7-BCD8-B4A3542CA60C</gtr:id><gtr:impact>We entered into discussion with a BBC journalist about covering our lab's work on VR.

The journalist has said he will follow this up with a program covering new developments in VR, particularly following the $2billion investment by Facebook in Oculus Rift</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>http://peterscarfe.com/bounceRecalibration.html</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>30982</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>The action-based brain: a provocation to philosophy, robotics and the cognitive sciences</gtr:description><gtr:end>2017-02-02</gtr:end><gtr:fundingOrg>Arts &amp; Humanities Research Council (AHRC)</gtr:fundingOrg><gtr:fundingRef>AH/N006011/1</gtr:fundingRef><gtr:id>1C69BEC8-FEB4-46F7-8B22-629A5D8A5820</gtr:id><gtr:sector>Public</gtr:sector><gtr:start>2016-03-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>443434</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Understanding Scenes and Events through Joint Parsing, Cognitive Reasoning and Lifelong Learning</gtr:description><gtr:end>2019-01-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/N019423/1</gtr:fundingRef><gtr:id>438EB1EF-0911-4A31-A585-B88F71D2617B</gtr:id><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-02-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>One paper published from this grant shows that sensory adaptation (in this case, people re-calibrate their sense of slant) depends on how they interpret physical interactions of objects (in this case a ball bouncing differently when it is spinning). This demonstrates that 'low-level cues' such as the range of slants people see, are not the only cause of adaptive changes in the visual system. We have also published a review paper on how to set up a high-fidelity virtual reality lab (at one point this was the most downloaded paper in Journal of Vision) and a theoretical paper on the problem of visual stability. A major review paper on the problem of spatial representation in a moving observer was published in Phil Trans B. This sets out a radical alternative to theories that suppose the brain builds 3D 'maps' or reconstructions of the scene. Another paper comparing view-based and reconstruction models as predictors of human navigation in a homing task has been reviewed and should be accepted for publication in Journal of Vision after minor revisions. Another paper will be submitted soon describing the errors that people make when they point at remembered objects after walking to a new location. We show that models based on an internal 3D reconstruction are poor at predicting human pointing in this task and describe a heuristic that predicts pointing more accurately.</gtr:description><gtr:exploitationPathways>We are collaborating with engineers who are interested in adaptive control systems. These results have implications for the design of such systems. We are now developing a more extensive collaboration with Professor Phil Torr's group the Department of Engineering in Oxford and with Professor Abhinav Gupta's lab in the Robotics Group at Carnegie Mellon University. The VR lab results will be used to inform and adapt machine learning techniques for learning spatial layout and will inform ideas about spatial representation in humans.</gtr:exploitationPathways><gtr:id>4F111113-7341-4A2C-ACAA-F739827840E4</gtr:id><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://peterscarfe.com/bounceRecalibration.html</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/E11180A1-24A4-4DFF-A68C-74FC1FDE8F56"><gtr:id>E11180A1-24A4-4DFF-A68C-74FC1FDE8F56</gtr:id><gtr:title>Humans use predictive kinematic models to calibrate visual cues to three-dimensional surface slant.</gtr:title><gtr:parentPublicationTitle>The Journal of neuroscience : the official journal of the Society for Neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/6403ef9e6e2f224e921aa27ea99266c6"><gtr:id>6403ef9e6e2f224e921aa27ea99266c6</gtr:id><gtr:otherNames>Scarfe P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0270-6474</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/D1A42B67-40FA-43AF-8DB1-FCE02D112690"><gtr:id>D1A42B67-40FA-43AF-8DB1-FCE02D112690</gtr:id><gtr:title>Using high-fidelity virtual reality to study perception in freely moving observers.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/6403ef9e6e2f224e921aa27ea99266c6"><gtr:id>6403ef9e6e2f224e921aa27ea99266c6</gtr:id><gtr:otherNames>Scarfe P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/7B7FF9C5-32DD-4BD3-A6D2-D08636A5F3DB"><gtr:id>7B7FF9C5-32DD-4BD3-A6D2-D08636A5F3DB</gtr:id><gtr:title>A moving observer in a three-dimensional world.</gtr:title><gtr:parentPublicationTitle>Philosophical transactions of the Royal Society of London. Series B, Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/cdd60f011d094818d0c8d752c2bf5814"><gtr:id>cdd60f011d094818d0c8d752c2bf5814</gtr:id><gtr:otherNames>Glennerster A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0962-8436</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/0FA70515-E67C-4AD5-A83D-921D64415729"><gtr:id>0FA70515-E67C-4AD5-A83D-921D64415729</gtr:id><gtr:title>Visual stability-what is the problem?</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/cdd60f011d094818d0c8d752c2bf5814"><gtr:id>cdd60f011d094818d0c8d752c2bf5814</gtr:id><gtr:otherNames>Glennerster A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K011766/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>