<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/68D0E3C9-9246-4CFC-B5E9-48584CF82993"><gtr:id>68D0E3C9-9246-4CFC-B5E9-48584CF82993</gtr:id><gtr:name>University of Manchester</gtr:name><gtr:department>Psychological Sciences</gtr:department><gtr:address><gtr:line1>Oxford Road</gtr:line1><gtr:city>Manchester</gtr:city><gtr:postCode>M13 9PL</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/68D0E3C9-9246-4CFC-B5E9-48584CF82993"><gtr:id>68D0E3C9-9246-4CFC-B5E9-48584CF82993</gtr:id><gtr:name>University of Manchester</gtr:name><gtr:address><gtr:line1>Oxford Road</gtr:line1><gtr:city>Manchester</gtr:city><gtr:postCode>M13 9PL</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/6AE6D6CD-7058-4E6A-801A-95FA183DD878"><gtr:id>6AE6D6CD-7058-4E6A-801A-95FA183DD878</gtr:id><gtr:firstName>Karen</gtr:firstName><gtr:surname>Lander</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=RES-000-23-1172"><gtr:id>82F0703C-5A5C-442D-B400-67A89DB1978D</gtr:id><gtr:title>I see what you say: Non-linguistic factors in speechreading</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>RES-000-23-1172</gtr:grantReference><gtr:abstractText>&lt;p>Visual information from a talker's mouth and face plays an important role in the perception and understanding of spoken language.&amp;nbsp;&lt;/p>

&lt;p>Despite the importance of speechreading, relatively little is known about why some speakers are easier to speechread than others. For example, is it easier to speechread a familiar face compared with an unfamiliar one?&amp;nbsp; Does the amount and manner of speech movement shown by the face affect speechreading performance?&amp;nbsp; What about the structural make-up of the face?&amp;nbsp; &lt;/p>

&lt;p>The aim of this new grant is to investigate these questions, focusing on non-linguistic factors that may influence why some faces are easier to speechread than others.&amp;nbsp; In this grant we aim to manipulate the familiarity of different faces and measure how easy they are to speechread.&amp;nbsp; In addition, we will compare the effects of speech manner and motion on the speechreadability of faces.&amp;nbsp; Finally we will investigate the importance of facial hair, lip and teeth visability on speechreading performance. Investigation of these topics is important for practical and theoretical reasons. &lt;/p>

&lt;p>Understanding of this issue should help maximise the effectiveness of faces in conveying visual speech information. This may be particularly important for increasing communication, both with and by, hearing impaired people.&lt;/p></gtr:abstractText><gtr:fund><gtr:end>2008-07-31</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2006-01-31</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>71257</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RES">RES-000-23-1172</gtr:identifier><gtr:identifier type="RCUK">RES-000-23-1172</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>