<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:department>School of Computer Science</gtr:department><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/ABA3AE56-6367-4175-B0FF-CDA79DEF6184"><gtr:id>ABA3AE56-6367-4175-B0FF-CDA79DEF6184</gtr:id><gtr:firstName>Nick</gtr:firstName><gtr:surname>Hawes</gtr:surname><gtr:orcidId>0000-0002-7556-6098</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/2BB701DD-9C61-4C03-97DE-7E1EA7F7B453"><gtr:id>2BB701DD-9C61-4C03-97DE-7E1EA7F7B453</gtr:id><gtr:firstName>Lars</gtr:firstName><gtr:surname>Kunze</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FM015777%2F1"><gtr:id>B3242BBF-1A92-4859-9655-FFB91511D599</gtr:id><gtr:title>ALOOF: Autonomous Learning of the Meaning of Objects</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M015777/1</gtr:grantReference><gtr:abstractText>When working with and for humans, robots and autonomous systems must know about the objects involved in human activities, e.g. the parts and tools in manufacturing, the professional items used in service applications, and the objects of daily life in assisted living. While great progress has been made in object instance and class recognition, a robot is always limited to knowing about the objects it has been trained to recognize. The goal of ALOOF is to enable robots to exploit the vast amount of knowledge on the Web in order to learn about previously unseen objects and to use this knowledge when acting in the real world. We will develop techniques to allow robots to use the Web to not just learn the appearance of new objects, but also their properties including where they might be found in the robot's environment. 

To achieve our goal, we will provide a mechanism for translating between the representations robots use in their real-world experience and those found on the Web. Our proposed translation mechanism is a meta-modal representation (i.e. a representation which contains and structures representations from other modalities), composed of meta-modal entities and relations between them. A single entity represents a single object type, and is composed of modal features extracted from robot sensors or the Web. The combined features are linked to the semantic properties associated with each entity. The robot's collection of meta-modal entities is organized into a structured ontology, supporting formal reasoning. This representation is complemented with methods for detecting gaps in the knowledge of the robot (i.e. unknown objects and properties), and for planning how to fill these gaps. As the robot's main source of new knowledge will be the Web, we will also contribute techniques for extracting relevant knowledge from Web resources using novel machine reading and computer vision algorithms.

By linking meta-modal representations with the perception and action capabilities of robots, we will achieve an innovative and powerful mix of Web-supported and physically-grounded life-long learning. Our scenario consists of an open-ended domestic setting where robots have to find objects. Our measure of progress will be how many knowledge gaps (i.e. situations where the robot has incomplete information about objects), can be resolved autonomously given specific prior knowledge. We will integrate the results on multiple mobile robots including the MetraLabs SCITOS robot, and the home service robot HOBBIT.</gtr:abstractText><gtr:potentialImpactText>ALOOF will change how we work with robots. Today, it is hardly believable that a robot could provide you with information about any object you show to it. After ALOOF we will be able to ask future robots to (1) find an object given a name, (2) find out the name for an unknown object and (3) find out other information about an object. ALOOF technology will enable any Web-connected device (e.g. from smart phones to robots) to learn from the Web, widening the range of applications that can profit from autonomous online knowledge gathering. This could range from driver assistance systems that could learn about locally unique buildings, signs, or sites, to Ambient Assistive Living (AAL) systems. In general, the capability to learn from the Web will become an essential tool for technology that must perform some task in loosely-structured and open-ended environments, thus creating impact (in terms of increased productivity and capability) in a wide range of fields, from supporting independent living to creating new shopping experiences.


In order to encourage the use of robots in new service scenarios, and to support the predicted growth in the service robot market, it is crucial that future service robots are autonomous and robust, and also as cheap and easy to deploy as possible. This cannot be said for current technology, as robots must be trained in advance for all objects they must work with (impossible in open-ended domains, as found in most service scenarios). ALOOF will change this. By using Web learning integrated with situated experience, robots can train themselves to recognize objects as they encounter them, thus minimising deployment overhead. By being founded on an approach that assumes missing knowledge is always part of a problem, ALOOF systems will be uniquely robust to failures in perception, using Web learning to fill in the gaps in their understanding. Successful demonstrations of these technologies will increase the uptake of robots based on UK technology in both targeted industries and beyond, increasing market share and UK competitiveness in the process.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-06-29</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2014-12-31</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>340805</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Harvey Nash Technology Survey launch event</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>DB5AC5FB-E4D7-4344-968B-A746799D80AA</gtr:id><gtr:impact>A talk on autonomous robots and AI at Harvey Nash Technology Survey launch event to tech industry CTOs and recruiters.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://www.harveynash.com/uk/news-and-media/practices-technology-news/harvey_nash_technology_survey_birmingham_2016_-_are_you_ready.asp</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Business Magazine Tech 100 after dinner talk</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>E1D3F0FF-62B0-43C7-A703-CCA2E2EA1ADC</gtr:id><gtr:impact>Talk to the Business Magazine Tech 100 launch event about autonomous systems and AI.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://www.businessmag.co.uk/southerntech100/southerntech100southern-tech-dinner-2016/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>University of the Third Age at the Royal Institution</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>A3CDE232-6D78-4E40-AEB0-768628BE7587</gtr:id><gtr:impact>A talk on autonomous robots to the University of the Third Age at the Royal Institution. My main topic was the (lack of an) existential threat from future AI and robots.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.rigb.org/whats-on/events-2015/october/public-u3a-at-the-ri-relativity-reproduction-and-robots</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Robotics Innovation Show</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>FB5FE7EF-BB21-4D54-908D-E96C33FBA20B</gtr:id><gtr:impact>Gave a talk on autonomous mobile service robots to the Robotics Innovation Show, a trade show for robots and autonomous systems. The talk led to some new relationships with partners interested in deploying autonomous service robots.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://www.roboticstoday.com/events/robotics-innovation-show</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Level Up Human podcast</gtr:description><gtr:form>A broadcast e.g. TV/radio/film/podcast (other than news/press)</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>71F020A6-4020-4DE9-92AB-BDB6BA7D0622</gtr:id><gtr:impact>Recording two episodes of the Level Up Human podcast, covering topics on AI and robotics.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://leveluphuman.com/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Proctor &amp; Gamble: Outside In</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>D80F8EFC-1A69-485C-8819-2DC9BFE9C581</gtr:id><gtr:impact>A talk on AI/Robotics for on of Proctor &amp;amp; Gamble's technology teams as part of their &amp;quot;Outside In&amp;quot; series of talks bringing in exciting external ideas.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Interview for Factor Magazine</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>0E3D4CD1-C4F9-4E46-B56C-E0E7FC65EDDC</gtr:id><gtr:impact>Interview for Factor magazine about Facebook founder Mark Zuckerberg building an AI-controlled butler.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://factor-tech.com/feature/zuckerberg-and-the-robot-butler/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BBC Radio 4's The Now Show</gtr:description><gtr:form>A broadcast e.g. TV/radio/film/podcast (other than news/press)</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>4453119C-2BE2-4B34-A7E8-5188F702C808</gtr:id><gtr:impact>Interview on the BBC Radio 4's The Now Show about DeepMind and AlphaGo.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/programmes/b072n8fq</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/112FB872-1A80-4450-8E60-C6F7743D55BC"><gtr:id>112FB872-1A80-4450-8E60-C6F7743D55BC</gtr:id><gtr:title>Towards Lifelong Object Learning by Integrating Situated Robot Perception and Semantic Web Mining</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/dd9dc19f58376ec7ea99cac38db7a810"><gtr:id>dd9dc19f58376ec7ea99cac38db7a810</gtr:id><gtr:otherNames>Young J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/E7E6A31B-1FB8-4CFB-B4D7-3118773EB816"><gtr:id>E7E6A31B-1FB8-4CFB-B4D7-3118773EB816</gtr:id><gtr:title>Autonomous Learning of Object Models on a Mobile Robot</gtr:title><gtr:parentPublicationTitle>IEEE Robotics and Automation Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/ea9c3bd7486ec49e2b1d101570cba56c"><gtr:id>ea9c3bd7486ec49e2b1d101570cba56c</gtr:id><gtr:otherNames>Faulhammer T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/D43568C9-014C-462E-B826-E66E9E94710C"><gtr:id>D43568C9-014C-462E-B826-E66E9E94710C</gtr:id><gtr:title>Semantic Web-Mining and Deep Vision for Lifelong Object Discovery</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/dd9dc19f58376ec7ea99cac38db7a810"><gtr:id>dd9dc19f58376ec7ea99cac38db7a810</gtr:id><gtr:otherNames>Young J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M015777/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0A982A4A-12CF-4734-AFCA-A5DC61F667F3</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Information &amp; Knowledge Mgmt</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>