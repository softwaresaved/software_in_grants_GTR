<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.rcuk.ac.uk:80/organisation/B65E5E61-8215-4F94-964C-E4207D4BCC23"><gtr:id>B65E5E61-8215-4F94-964C-E4207D4BCC23</gtr:id><gtr:name>Environmental Resources Management</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:department>Physiology Development and Neuroscience</gtr:department><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/536116AC-C155-4A24-A743-309BD68E50CE"><gtr:id>536116AC-C155-4A24-A743-309BD68E50CE</gtr:id><gtr:name>Defence Science &amp; Tech Lab DSTL</gtr:name><gtr:address><gtr:line1>Defence Science &amp; Tech Lab - MOD</gtr:line1><gtr:line2>Porton Down</gtr:line2><gtr:line4>Salisbury</gtr:line4><gtr:postCode>SP4 0JQ</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/B65E5E61-8215-4F94-964C-E4207D4BCC23"><gtr:id>B65E5E61-8215-4F94-964C-E4207D4BCC23</gtr:id><gtr:name>Environmental Resources Management</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/8DAD57CA-8364-4A60-8014-9FD247A262F9"><gtr:id>8DAD57CA-8364-4A60-8014-9FD247A262F9</gtr:id><gtr:firstName>David John</gtr:firstName><gtr:surname>Tolhurst</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FE037097%2F1"><gtr:id>96C92DC7-2552-434D-9378-48767EAFF34B</gtr:id><gtr:title>Natural dynamic scenes and human vision</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E037097/1</gtr:grantReference><gtr:abstractText>There is a widespread, and reasonable, assumption that our visual system has developed to see and interpret our natural surroundings. This has been investigated in the past by studying the relationship between the structure of natural scenes and the properties of biological systems looking at those scenes. However, much past work has suffered from two assumptions known to be false: first, that nothing moves in the scenes, and second, that the observers do not move their eyes. The reason for this over-simplification has been the technical difficulty of adding these important variables. We have assembled a team of researchers in two Universities (Bristol and Cambridge) who, together, have the necessary expertise to take on this task. We will collect a large number of images and video clips of outdoor scenes in which there is natural movement, such as leaves rustling in the wind, or of objects in motion. We will study how the information in these scenes is encoded by the visual brain, both with theoretical models and with experiments involving human observers looking at the video clips and having to decide whether or not successive video clips are the same, or different from each other.What makes the modelling challenging is the second issue to be explored here - namely, that we move our gaze to a particular place because only one part of our retina, the fovea, has high spatial resolution. The eye movements that we make provide us with sequential information about a scene. We want our model to know (a) how this information is taken up when the eye is looking at one place (it is said to be fixating ), and (b) how it is combined with information from previous and future fixation locations. In other words, how does vision integrate information across eye movements?The novelty of this work is manifold. First, we will calibrate video and still cameras to obtain accurate images of natural scenes from which we can work out how human cones at each location would respond when looking at the scene. There has not been a study of the time-varying properties of natural scenes, and we will provide a resource both for this study and other interested researchers. Furthermore, we will study the interplay between fixation and information uptake and storage in human vision, for natural scenes. Finally, we will develop a computational model capable of predicting to what extent human observers will notice differences between two scenes when they move their eyes, and when scenes contain movement. Such a model is useful for many applications, such as measuring whether people will notice errors in the quality of graphics images, and for estimating the degree to which people will notice the presence of camouflaged objects in the scene.</gtr:abstractText><gtr:fund><gtr:end>2010-07-31</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-08-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>292021</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Environmental Resources Management</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>commercial collaboration</gtr:description><gtr:id>9438506C-9107-466A-8E99-42E0F589C2C1</gtr:id><gtr:impact>no formal outputs</gtr:impact><gtr:partnerContribution>evaluate software package</gtr:partnerContribution><gtr:piContribution>Advisory through ERM of signal and sign placing for Network Rail</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2007-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Advisory to DstL and Network Rail on visual conspicuity of targets and signage.

Our Vision reaserach on complex natural scenes led to the development of a computer model of human perception of natural scenes. that software can be used to estimate the conspicuity of, e.g., critical rail signs and we can advise on their placement and organisation.

Beneficiaries: MoD/Dstl; Network Rail

Contribution Method: advisory and development of software algorithms to guide placement of signage or to study conspicuity of targets</gtr:description><gtr:firstYearOfImpact>2008</gtr:firstYearOfImpact><gtr:id>AE047A2C-AC3A-4F1E-8374-7A40B9729B1F</gtr:id><gtr:impactTypes><gtr:impactType>Societal</gtr:impactType></gtr:impactTypes><gtr:sector>Aerospace, Defence and Marine,Environment</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The project has sought to develop rigorous experimental protocols for studying human perception of features in photographs of natural scenes. This is a big step beyond the usual laboratory studies of human vision, which use simplified visual stimuli which do not relate directly to everday visual experience. From our experiments, we have attempted to build a computer model of how nerve cells in the human brain each respond to features in natural scenes so that we can predict which features or changes will be visible and which will be invisible. In this project, we have specifically compared direct, foveal vision with peripheral vision which conveys less detail; we have examined how people evaluate differences between movie clips of similar events; and we have extended the study of &amp;quot;visual search&amp;quot; to understand how people search for natural targets.</gtr:description><gtr:exploitationPathways>This has clear applications in defence for trying to evaluate whether particular camouglage schemes might be more or less effective than others. From working (indirectlt) with Network rail on the conspicuity of railside signage, we expect that the software package could be extended to study the visibility of signage in many sectors, or the visibility of safety-critical situations such as railway crossings. Our work has been published in typical peer review scientific journals; but we have also developed software packages for use by commercial partners who may wish to estimate how well human observers might see items of importance. The software examines colour photographs of scenes and then attempts to model how the visual coding in the brain would perceive the presence or absence of key features.</gtr:exploitationPathways><gtr:id>67909060-F513-4C73-BDBA-6C719D9ADDDA</gtr:id><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Digital/Communication/Information Technologies (including Software),Environment,Transport</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/8CD22A38-0631-4DC0-9281-4FAB7DDF059F"><gtr:id>8CD22A38-0631-4DC0-9281-4FAB7DDF059F</gtr:id><gtr:title>Perception of differences in naturalistic dynamic scenes, and a V1-based model.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/a85e83acf53a04346d42fb4f53556f21"><gtr:id>a85e83acf53a04346d42fb4f53556f21</gtr:id><gtr:otherNames>To MP</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/2D128DA7-DAF6-44A1-93B1-2D382791BF00"><gtr:id>2D128DA7-DAF6-44A1-93B1-2D382791BF00</gtr:id><gtr:title>Discrimination of natural scenes in central and peripheral vision.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/a85e83acf53a04346d42fb4f53556f21"><gtr:id>a85e83acf53a04346d42fb4f53556f21</gtr:id><gtr:otherNames>To MP</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/151F0284-1DC1-458D-883B-66C30D86DAAF"><gtr:id>151F0284-1DC1-458D-883B-66C30D86DAAF</gtr:id><gtr:title>A general rule for sensory cue summation: evidence from photographic, musical, phonetic and cross-modal stimuli.</gtr:title><gtr:parentPublicationTitle>Proceedings. Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/a85e83acf53a04346d42fb4f53556f21"><gtr:id>a85e83acf53a04346d42fb4f53556f21</gtr:id><gtr:otherNames>To MP</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0962-8452</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/73993CCA-3817-42A3-9B6A-7EE45AB7E358"><gtr:id>73993CCA-3817-42A3-9B6A-7EE45AB7E358</gtr:id><gtr:title>Camouflage and visual perception.</gtr:title><gtr:parentPublicationTitle>Philosophical transactions of the Royal Society of London. Series B, Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/0ec31868382c1a7f5334244d3294bcd4"><gtr:id>0ec31868382c1a7f5334244d3294bcd4</gtr:id><gtr:otherNames>Troscianko T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>0962-8436</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/535A16B9-30AB-4FD1-B036-BD2ED3C462BA"><gtr:id>535A16B9-30AB-4FD1-B036-BD2ED3C462BA</gtr:id><gtr:title>Perception of differences in natural-image stimuli</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Applied Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/3726c6d84f259139df5ad90f1e82f928"><gtr:id>3726c6d84f259139df5ad90f1e82f928</gtr:id><gtr:otherNames>To M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/B8946DA7-288A-4D35-9111-001D2E0A3111"><gtr:id>B8946DA7-288A-4D35-9111-001D2E0A3111</gtr:id><gtr:title>Search for gross illumination discrepancies in images of natural objects</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/339f1f3d64618fe43d1190d7acbd7dcf"><gtr:id>339f1f3d64618fe43d1190d7acbd7dcf</gtr:id><gtr:otherNames>Lovell P. G.</gtr:otherNames></gtr:author></gtr:authors></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/3774810E-C12F-478B-A578-A607B34AA0ED"><gtr:id>3774810E-C12F-478B-A578-A607B34AA0ED</gtr:id><gtr:title>Summation of perceptual cues in natural visual scenes.</gtr:title><gtr:parentPublicationTitle>Proceedings. Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/3726c6d84f259139df5ad90f1e82f928"><gtr:id>3726c6d84f259139df5ad90f1e82f928</gtr:id><gtr:otherNames>To M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:issn>0962-8452</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/9E5D4587-0D54-4EA1-9AE1-62CCFDB677C5"><gtr:id>9E5D4587-0D54-4EA1-9AE1-62CCFDB677C5</gtr:id><gtr:title>Perception of suprathreshold naturalistic changes in colored natural images.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/a85e83acf53a04346d42fb4f53556f21"><gtr:id>a85e83acf53a04346d42fb4f53556f21</gtr:id><gtr:otherNames>To MP</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/5EBE7C5C-7CD7-467C-B706-8D5FBF9DE895"><gtr:id>5EBE7C5C-7CD7-467C-B706-8D5FBF9DE895</gtr:id><gtr:title>Perception of differences in naturalistic dynamic scenes, and a V1-based model.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/a85e83acf53a04346d42fb4f53556f21"><gtr:id>a85e83acf53a04346d42fb4f53556f21</gtr:id><gtr:otherNames>To MP</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E037097/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>