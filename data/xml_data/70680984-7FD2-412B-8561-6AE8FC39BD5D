<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.rcuk.ac.uk:80/organisation/BFE5AEC9-2D01-4940-8F16-53E7703979C2"><gtr:id>BFE5AEC9-2D01-4940-8F16-53E7703979C2</gtr:id><gtr:name>Manchester City Council</gtr:name><gtr:address><gtr:line1>PO Box 643, Town Hall</gtr:line1><gtr:line2>Albert Square</gtr:line2><gtr:line4>Manchester</gtr:line4><gtr:postCode>M60 3NY</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/83D87776-5958-42AE-889D-B8AECF16B468"><gtr:id>83D87776-5958-42AE-889D-B8AECF16B468</gtr:id><gtr:name>University of Leeds</gtr:name><gtr:department>Sch of Computing</gtr:department><gtr:address><gtr:line1>University of Leeds</gtr:line1><gtr:line4>Leeds</gtr:line4><gtr:line5>West Yorkshire</gtr:line5><gtr:postCode>LS2 9JT</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/83D87776-5958-42AE-889D-B8AECF16B468"><gtr:id>83D87776-5958-42AE-889D-B8AECF16B468</gtr:id><gtr:name>University of Leeds</gtr:name><gtr:address><gtr:line1>University of Leeds</gtr:line1><gtr:line4>Leeds</gtr:line4><gtr:line5>West Yorkshire</gtr:line5><gtr:postCode>LS2 9JT</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/F11E21C9-E05F-4173-939C-61F868FE1FBA"><gtr:id>F11E21C9-E05F-4173-939C-61F868FE1FBA</gtr:id><gtr:name>The Wellcome Trust Ltd</gtr:name><gtr:address><gtr:line1>215 Euston Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>NW1 2BE</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/BFE5AEC9-2D01-4940-8F16-53E7703979C2"><gtr:id>BFE5AEC9-2D01-4940-8F16-53E7703979C2</gtr:id><gtr:name>Manchester City Council</gtr:name><gtr:address><gtr:line1>PO Box 643, Town Hall</gtr:line1><gtr:line2>Albert Square</gtr:line2><gtr:line4>Manchester</gtr:line4><gtr:postCode>M60 3NY</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/DE0CEBB6-9C5F-4FEC-8714-B13526DCB21B"><gtr:id>DE0CEBB6-9C5F-4FEC-8714-B13526DCB21B</gtr:id><gtr:firstName>David</gtr:firstName><gtr:surname>Hogg</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FE010164%2F1"><gtr:id>70680984-7FD2-412B-8561-6AE8FC39BD5D</gtr:id><gtr:title>Cognitive Systems Foresight: Human Attention and Machine Learning</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E010164/1</gtr:grantReference><gtr:abstractText>Human observers move their eyes in order to direct their attention to important aspects of a visual scene. There are models called salience maps; they predict where the eyes will move to when looking at a scene. At present, these models do not deal with video input, nor do they predict how an observer's task will affect where they look. In other words, there are no models for real-life viewing situations, where an observer has a specific task.We are proposing a new approach to this problem. We have access to video information from cameras used in urban surveillance, and to the operators whose job it is to spot abnormal behaviour in such video inputs. We shall obtain (previously unseen) video recordings of events in UK urban streets, and display them in a simulated control room to operators familiar with the town in question. We shall monitor where they look on the bank of video screens, and also when they decide that an event is abnormal and/or requires some form of intervention, e.g. calling the police. We shall use the record of eye fixations to teach a computer system to distinguish between normal and abnormal events. In this way, we shall be able to learn what is important for humans to do such surveillance by observing their eye fixation behaviour, for a realistic (and difficult) task and set of real-life video sequences. The project is important for four reasons. First, this will be the first attempt to develop a model of human attention/eye movements which will be firmly based on realistic video input and a real task. Second, this will be the first time that a computer system is able to learn from human behaviour in this way. Third, we will learn much about the ability of trained observers to cope with a demanding task as the number of TV monitors increases. Finally, we will develop an automated system which will be able to analyse the input from any urban CCTV camera in order to alert operators to look at that video stream - at present, most CCTV video streams are not observed by anyone since there are too many cameras for the number of human observers. Therefore, an automated alerting system is greatly neeeded and this project constitutes the best attempt to date to produce one.</gtr:abstractText><gtr:fund><gtr:end>2010-01-31</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-02-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>334071</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Manchester City Council</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Manchester City Council</gtr:description><gtr:id>210C9336-E1D2-43F4-A9BE-43CB3E2C8AEF</gtr:id><gtr:sector>Public</gtr:sector><gtr:start>2007-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/C6CDCFA3-97A9-4989-ADBC-8C11629526C9"><gtr:id>C6CDCFA3-97A9-4989-ADBC-8C11629526C9</gtr:id><gtr:title>Suspiciousness perception in dynamic scenes: a comparison of CCTV operators and novices.</gtr:title><gtr:parentPublicationTitle>Frontiers in human neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/7ba05a5f152fd01a291efd822cf39df1"><gtr:id>7ba05a5f152fd01a291efd822cf39df1</gtr:id><gtr:otherNames>Howard CJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1662-5161</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/C502CCB5-430A-4D6A-8031-A6BB65621530"><gtr:id>C502CCB5-430A-4D6A-8031-A6BB65621530</gtr:id><gtr:title>Implicit color segmentation features for pedestrian and object detection</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/b4c78c7e5736c47de945dea9c328bf6b"><gtr:id>b4c78c7e5736c47de945dea9c328bf6b</gtr:id><gtr:otherNames>Ott P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-4420-5</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/0F37E89C-6103-4B00-86A0-ECC409FDD33E"><gtr:id>0F37E89C-6103-4B00-86A0-ECC409FDD33E</gtr:id><gtr:title>Human Factors Security and Safety</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/69f951de0436691437dfdad7f814ce40"><gtr:id>69f951de0436691437dfdad7f814ce40</gtr:id><gtr:otherNames>Howard, C J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-90-423-0373-7</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/CA89D16F-251E-4EFF-A908-823AA654623B"><gtr:id>CA89D16F-251E-4EFF-A908-823AA654623B</gtr:id><gtr:title>Task relevance predicts gaze in videos of real moving scenes.</gtr:title><gtr:parentPublicationTitle>Experimental brain research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/7ba05a5f152fd01a291efd822cf39df1"><gtr:id>7ba05a5f152fd01a291efd822cf39df1</gtr:id><gtr:otherNames>Howard CJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0014-4819</gtr:issn></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E010164/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>CC2B62EB-22CD-45F9-A6D2-0CE29B6D90FD</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Cognitive Science Appl. in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>