<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.rcuk.ac.uk:80/organisation/B936D768-A528-4ABB-A6FA-D5F91D6FD0F4"><gtr:id>B936D768-A528-4ABB-A6FA-D5F91D6FD0F4</gtr:id><gtr:name>Defence Science &amp; Technology Laboratory (DSTL)</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/936D002F-A8D1-4A93-AE5D-825ED0903D8D"><gtr:id>936D002F-A8D1-4A93-AE5D-825ED0903D8D</gtr:id><gtr:name>University of Nottingham</gtr:name><gtr:department>School of Computer Science</gtr:department><gtr:address><gtr:line1>University Park</gtr:line1><gtr:line4>Nottingham</gtr:line4><gtr:line5>Nottinghamshire</gtr:line5><gtr:postCode>NG7 2RD</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/936D002F-A8D1-4A93-AE5D-825ED0903D8D"><gtr:id>936D002F-A8D1-4A93-AE5D-825ED0903D8D</gtr:id><gtr:name>University of Nottingham</gtr:name><gtr:address><gtr:line1>University Park</gtr:line1><gtr:line4>Nottingham</gtr:line4><gtr:line5>Nottinghamshire</gtr:line5><gtr:postCode>NG7 2RD</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/B936D768-A528-4ABB-A6FA-D5F91D6FD0F4"><gtr:id>B936D768-A528-4ABB-A6FA-D5F91D6FD0F4</gtr:id><gtr:name>Defence Science &amp; Technology Laboratory (DSTL)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/AB1E764B-ADD6-456D-8F2C-1BE31CFEB2E5"><gtr:id>AB1E764B-ADD6-456D-8F2C-1BE31CFEB2E5</gtr:id><gtr:firstName>Jonathan</gtr:firstName><gtr:otherNames>Mark</gtr:otherNames><gtr:surname>Garibaldi</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/35032571-C626-443C-B184-378F7EAE0F9A"><gtr:id>35032571-C626-443C-B184-378F7EAE0F9A</gtr:id><gtr:firstName>Guoping</gtr:firstName><gtr:surname>Qiu</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/37117512-5047-4EB4-AA8E-F2475FB9D5FE"><gtr:id>37117512-5047-4EB4-AA8E-F2475FB9D5FE</gtr:id><gtr:firstName>Uwe</gtr:firstName><gtr:surname>Aickelin</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FJ020257%2F1"><gtr:id>901987B1-21D8-4D34-BA61-F62C621FBC97</gtr:id><gtr:title>FISH: Fast Semantic Nearest Neighbour Search</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/J020257/1</gtr:grantReference><gtr:abstractText>This proposal addresses Theme 1 of the Data Intensive Systems (DaISy) Call &amp;quot;Extracting meaningful information: Deriving meaning from large, heterogeneous, incomplete, contradictory, noisy and dispersed data sets with many different forms and formats (e.g. text, image and sound files)&amp;quot;. The project will research and develop novel fast semantic nearest neighbour search (FISH) data structure, algorithm and software utilities for rapidly discovering semantically similar neighbours of complex data objects and automatically assigning semantic class labels to them. It will use large scale, high dimensional, heterogeneous, incomplete, and noisy internet image labelling datasets as Case Study for technology development and evaluation. The FISH solutions and utilities can be directly applied to automatically label security surveillance videos and defence reconnaissance imageries to extract semantic meanings to facilitate security and defence intelligence gathering and analysis.</gtr:abstractText><gtr:potentialImpactText>Defence reconnaissance systems such as the RAF's RAPTOR contain imaging sensors (visible and infrared) which can acquire huge volumes of high resolution imageries of ground targets. Rapidly and accurately interpreting the images, identifying and recognizing ground objects and assessing the battle-field situations will be very important. Post-battle analysis of these imageries will be also valuable for future operations. Manually analysing these large volumes of high resolution imageries can be both difficult and labour intensive. Automatic image analysis and interpretation will play a very important role in making good use of defence reconnaissance data such as those acquired by the RAF's RAPTOR. The research results of this project can be useful for these purposes. 

The FISH algorithms and solutions can be applied to automatic labelling of the images and videos returned by RAPTOR. There are at least two scenarios where FISH can be useful. It can be implemented onboard the aircraft interpreting and labelling the images in real-time thus helping the pilots assessing the battle-field situations and react to them more rapidly. It can also be implemented in ground stations for post battle analysis and for archiving - by automatically labelling the images of different situations can facilitate search and retrieval of specific and relevant situations. 

There are an estimated 1.85 million surveillance video cameras in the UK constantly collecting billions of image footages which could contain vital security information for antiterrorism, for preventing crime and for protecting human lives. For such huge volumes of data, it will be impossible to interpret them manually. Automatic tools will be valuable and FISH can help develop such tools. 

The FISH algorithms and solutions can be used to automatically label security surveillance videos, identify dangerous and emergency situations quickly thus helping security authorities to gather security intelligence and react to them fast. Such techniques can be employed by the police and other security authorities for real-time situation monitoring or for post-event analysis - semantically labelled video frames can greatly facilitate the retrieval of specific and relevant contents. 

For defence, the FISH technology can help extract meanings from imageries acquired by reconnaissance systems such as RAF's RAPTOR thus facilitating defence intelligence gathering. For security, the FISH technology can help extract meanings from security videos thus helping gathering security information which may be valuable for antiterrorism, preventing crime and saving lives. 

In summary, the research of the FISH project can contribute state of the art technology for improving defence and security intelligence gathering and analysis which will lead to improvement in the UK's defence and security capability. 

Beyond defence and security, the research of the FISH project will have wider applications in academic research, and in applications fields ranging from biomedical to multimedia and the internet. 

In biomedical research, e.g., disease classification in medical images and general biomedical data classification and analysis such as matching DNA sequences involve finding similar data objects, the FISH technology can have a direct applications in these areas.</gtr:potentialImpactText><gtr:fund><gtr:end>2013-09-30</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2012-06-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>95625</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Defence Science &amp; Technology Laboratory (DSTL)</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>DSTL</gtr:description><gtr:id>053D113D-8B2B-48F5-8713-5CC20EB3431A</gtr:id><gtr:impact>(1) Random forest for image annotation
(2) DOI: 10.1016/j.ecoinf.2013.08.002
(3) doi&amp;gt;10.1145/2393347.2396344</gtr:impact><gtr:partnerContribution>Intellectual input, potential applications of the new technologies developed in the project</gtr:partnerContribution><gtr:piContribution>New automatic image labeling methods for assigning semantic meanings (labels) to visual images</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The automatics image labeling method developed through this grant has found new applications in automatic habitat classification. Researchers from the British Ordinance Survey have shown great interest in applying the technologies to automatically categories habitats based on GPS-referenced ground photographs. Using data supplied by Ordinance Survey, we have successfully demonstrated the application of the Fast Semantic Nearest Neighbour Search (FISH) technologies developed in this grant to automatically classify habitats (Ecological Informatics, vol. 23, pp. 126-136, 2014). Habitat classification is important for monitoring the environment and biodiversity. Traditionally, this is done manually by human surveyors, a laborious, expensive and subjective process. Automatic habitat classification has the advantage and potential of improving efficiency, reducing cost, and removing subjectivity. Therefore, the grant has the potential impact of assisting environment protection as well as improving efficiency thus producing positive economy and societal impacts.</gtr:description><gtr:firstYearOfImpact>2013</gtr:firstYearOfImpact><gtr:id>807D292D-DE0A-408C-9844-D06AABD64667</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:sector>Digital/Communication/Information Technologies (including Software),Environment</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Nearest neighbour search is fundamental to data analysis. In the &amp;quot;big data&amp;quot; era, advanced data analysis technologies and tools such as nearest neighbour search are fundamentally important. In this project we have made significant advances through developing the Fast Semantic Nearest Neighbour Search (FISH) technologies, in particular we have developed the following new technologies and applications

(1) A random forest based fast semantic nearest neighbour search method that tackles two fundamental drawbacks of traditional methods: scalability and semantic gap. The new method is based on tree data structure which is intrinsically fast and scalable. The new method finds neighbours that are not only close in the feature space but also have similar semantic meanings overcoming traditional methods that can only find neighbours that are close in the feature space but have very different semantic meanings. This is a new generic nearest neighbour search technique that overcomes the two fundamental weaknesses of traditional methods. 

(2) The new FISH method has been successfully applied to automatically label images and videos with what they contains. For example, for an image of a harbor taken during sunset, our new algorithm can label it with the meaningful words such as &amp;quot;sunset&amp;quot;, &amp;quot;water&amp;quot;, &amp;quot;boat&amp;quot;, etc. This is very useful for managing large image databases and for search relevant images on the Internet. 

(3) New methods based on FISH for automatically classify habitants. This is the first time that image-labeling technology has been applied to automatically label ground photographs with the habitats they contain. This has open up the new possibility of using computer algorithms to replace traditional labour intensive approach to habitat classification.</gtr:description><gtr:exploitationPathways>The FISH technologies can be programmed into a data analysis tool for finding semantic nearest neighbours in generic data analysis tasks. 

The FISH technologies can be programmed into image analysis tools for automatically labeling images and videos with what they contain, and for searching semantically similar images from the Internet or large image repositories.

The FISH technologies can be programmed to build automatic habitat classification tools. Software tools based on FISH can take GPS-referenced ground photographs as input and automatically label them with the habits they contain, thus automatically label the habitats of geographical locations.</gtr:exploitationPathways><gtr:id>021A0216-D6E0-4409-AB8E-B94ED25DDD3B</gtr:id><gtr:sectors><gtr:sector>Agriculture, Food and Drink,Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Environment,Financial Services, and Management Consultancy,Culture, Heritage, Museums and Collections,Security and Diplomacy,Transport</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/C6787E1F-AF15-4753-B3BB-93B58C0AFD28"><gtr:id>C6787E1F-AF15-4753-B3BB-93B58C0AFD28</gtr:id><gtr:title>An Efficient Gland Detection Method Based on Texture and Morphological Transformation</gtr:title><gtr:parentPublicationTitle>Medical Image Understanding and Analysis</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/3446894423d4e37d82c73fa8830e3b06"><gtr:id>3446894423d4e37d82c73fa8830e3b06</gtr:id><gtr:otherNames>Jie, S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/B03054CA-63D5-4F47-AC0B-88A2E9C65223"><gtr:id>B03054CA-63D5-4F47-AC0B-88A2E9C65223</gtr:id><gtr:title>Tree partition voting min-hash for partial duplicate image discovery</gtr:title><gtr:parentPublicationTitle>2013 IEEE Conference on Multimedia and Expo</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/4a788592078936e8b72ad49808cab76c"><gtr:id>4a788592078936e8b72ad49808cab76c</gtr:id><gtr:otherNames>Zhang, Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/6F924F07-759E-4C1E-B230-985AB441E8BE"><gtr:id>6F924F07-759E-4C1E-B230-985AB441E8BE</gtr:id><gtr:title>Computer Vision - ECCV 2012</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/bb9606206151d22b19e9270d672f5575"><gtr:id>bb9606206151d22b19e9270d672f5575</gtr:id><gtr:otherNames>Fu H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-3-642-33782-6</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/FFE32E85-DC71-4417-811D-80D5BA71A7FC"><gtr:id>FFE32E85-DC71-4417-811D-80D5BA71A7FC</gtr:id><gtr:title>Segmenting overlapping cell nuclei in digital histopathology images</gtr:title><gtr:parentPublicationTitle>Engineering in medicine and biology society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/3446894423d4e37d82c73fa8830e3b06"><gtr:id>3446894423d4e37d82c73fa8830e3b06</gtr:id><gtr:otherNames>Jie, S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/83E96C32-3164-40D0-9C85-A773BFCA6C01"><gtr:id>83E96C32-3164-40D0-9C85-A773BFCA6C01</gtr:id><gtr:title>Face hallucination based on sparse local-pixel structure</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/bd75dbe105014309cbdded985198a393"><gtr:id>bd75dbe105014309cbdded985198a393</gtr:id><gtr:otherNames>Li Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/4EE9D6B5-C8AB-4293-A750-FAF343C6A935"><gtr:id>4EE9D6B5-C8AB-4293-A750-FAF343C6A935</gtr:id><gtr:title>Fast semantic image retrieval based on random forest</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/bb9606206151d22b19e9270d672f5575"><gtr:id>bb9606206151d22b19e9270d672f5575</gtr:id><gtr:otherNames>Fu H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>9781450310895</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/A5F60B38-FEE1-4A01-8A5F-6D070A576063"><gtr:id>A5F60B38-FEE1-4A01-8A5F-6D070A576063</gtr:id><gtr:title>Habitat classification using random forest based image annotation</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/9f04c6bdd09d03d754dfae5989a934f9"><gtr:id>9f04c6bdd09d03d754dfae5989a934f9</gtr:id><gtr:otherNames>Torres M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/4421B64C-FDB0-4BC6-A6CC-38D6C5E49A12"><gtr:id>4421B64C-FDB0-4BC6-A6CC-38D6C5E49A12</gtr:id><gtr:title>Automatic habitat classification using image analysis and random forest</gtr:title><gtr:parentPublicationTitle>Ecological Informatics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/9f04c6bdd09d03d754dfae5989a934f9"><gtr:id>9f04c6bdd09d03d754dfae5989a934f9</gtr:id><gtr:otherNames>Torres M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/DC3B4F1F-2437-4615-9D09-C6E19752227C"><gtr:id>DC3B4F1F-2437-4615-9D09-C6E19752227C</gtr:id><gtr:title>Grass, scrub, trees and random forest</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/9f04c6bdd09d03d754dfae5989a934f9"><gtr:id>9f04c6bdd09d03d754dfae5989a934f9</gtr:id><gtr:otherNames>Torres M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>9781450315883</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/9FF7EC93-16FC-4395-AEC6-AAA7D3424583"><gtr:id>9FF7EC93-16FC-4395-AEC6-AAA7D3424583</gtr:id><gtr:title>Statistical colour models: an automated digital image analysis method for quantification of histological biomarkers.</gtr:title><gtr:parentPublicationTitle>Biomedical engineering online</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/4a6c96c5826d9d091908263fa1f740c4"><gtr:id>4a6c96c5826d9d091908263fa1f740c4</gtr:id><gtr:otherNames>Shu J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1475-925X</gtr:issn></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/J020257/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0A982A4A-12CF-4734-AFCA-A5DC61F667F3</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Information &amp; Knowledge Mgmt</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>