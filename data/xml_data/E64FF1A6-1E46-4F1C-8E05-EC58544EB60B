<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:department>College of Medical, Veterinary &amp;Life Sci</gtr:department><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/4C5F14F0-9C55-4D77-B895-C2238785E836"><gtr:id>4C5F14F0-9C55-4D77-B895-C2238785E836</gtr:id><gtr:firstName>Rachael</gtr:firstName><gtr:otherNames>Elizabeth</gtr:otherNames><gtr:surname>Jack</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/BA1958FA-5D03-43A8-8CB2-469390168B8A"><gtr:id>BA1958FA-5D03-43A8-8CB2-469390168B8A</gtr:id><gtr:firstName>Philippe</gtr:firstName><gtr:otherNames>Georges</gtr:otherNames><gtr:surname>Schyns</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=ES%2FK00607X%2F1"><gtr:id>E64FF1A6-1E46-4F1C-8E05-EC58544EB60B</gtr:id><gtr:title>Data-driven Analysis of the Dynamics of Information-acquisition Over time during Social judgement</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/K00607X/1</gtr:grantReference><gtr:abstractText>&lt;p>At the heart of human society, social interactions shape and maintain the complex network of relation- ships between individuals and groups. Yet, whereas some social exchanges foster healthy relations with considerable benefits for society, some actively jeopardise the social connections on which harmonious societies are based.&lt;/p>

&lt;p>A powerful driver of such social degeneration is prejudice, reflected by the stereotypes harbored by individuals. Specifically, stereotypes bias thought and action via top-down processes, including imposing unsubstantiated inferences about competence, trustworthiness and criminality based on the recognition of out-group identifiers.&lt;/p>

&lt;p>However, little is known about how stereotypes and prejudice exert a top-down control to shape the dynamics of information extraction and integration, thereby influencing critical social judgments. We propose a state-of-the-art data-driven and modeling analysis of the dynamics of information acquisition over the time course of social judgments. We will document the role of top-down factors on the extraction dynamics and use of information for social judgments by combining eye movements and reverse correlation methods to understand how top-down social categories (prejudice and stereotypes) modulate the sequences of eye fixations on static faces to extract social information for social judgments. &lt;/p></gtr:abstractText><gtr:potentialImpactText>Who will benefit from this research?
- Academic beneficiaries. The Social Sciences and Neuroscience, Information and Communication Technology (ICT).
- Industrial stakeholders. Gaming industry, robotics, social networking industry. More generally, the exploding field of social computing, with higher-level parameterisation of face and better under-standing of how the face communicates important social dimensions. 
- User community. Intuitive, socially intelligent interfaces for mediated communication. Older population, companion robots. Wider impact of social networking on society.

How will they benefit from this research? 
- Academic 
o Social Sciences and Neuroscience. Dimensions of physical attractiveness, dominance and trustworthiness have primary importance for social judgements that directly influence the lives of individuals. Determining precisely the integration of information causing these judgements is centrally important for social science. Normative models of social signals de-rived in our research will also inform how populations with social deficits fail (e.g. Autism Spectrum Disorder and Asperger Syndrome). The Glasgow group currently collaborates on this topic with Ralph Adolphs (CalTech, USA) to develop more effective remediation thera-pies. A previous related collaboration using our methods produced a Nature publication in 2005.
o ICT. With the development of the digital economy, avatars and companion robots - i.e., re-alistic and generalisable systems for automatically expressing and reading social signals depend on accurate models of these signals. This is not only important for producing modulated signals to communicate with precision, but also to read signals of varying intensities for accurate interpretation. Our research directly addresses these issues and has been very well received in the ICT community due to the flexibility of our 4D platform (i.e. it can derive models of varying intensity for any social signal arising from facial movements) and also because our 4D models are intrinsically and uniquely grounded in human perception, thus validating them.
- Industry. The gaming and 3D-movie industries rely on detailed understandings of social signals. Their automated recognition ranges from homeland security to remote communication via avatars and companion robots, with automated expression applying primarily to the latter two. So far, the 3D-movie/animation industries use point-light displays to capture social signals. Yet, our 4D plat-form provides models with a far greater power of generalisation.
- User community. Ultimately, the user community will benefit from ecologically improved (i.e., more human), more intelligent communication interfaces for mediated communication based on socially-relevant traits.</gtr:potentialImpactText><gtr:fund><gtr:end>2015-08-31</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2012-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>326025</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our findings are dynamic mathematical models of facial expressions of emotion, mental states and social traits. These will be used in the digital economy, to synthesize perceptually validated and culturally sensitive facial expressions on an avatar. Specifically, we have engaged with Furhat Robotics (http://www.furhatrobotics.com) and are currently transferring our findings to their social robot. Simply stated, this involves mapping our dynamic facial expressions onto the face of the Furhat robot and validate their accurate categorization. This is on-going work and we will update in due time.</gtr:description><gtr:firstYearOfImpact>2013</gtr:firstYearOfImpact><gtr:id>A608D749-7C29-4750-889E-4804BE90240B</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>In human social interaction, the face is a central tool of communication because it provides a rich source of social information. Although some signals (e.g., facial expressions of emotion and mental states) can be voluntarily deployed strategically to negotiate social situations, other signals (e.g., those indicating social traits such as dominance, trustworthiness, and attractiveness) are transmitted involuntarily by the phenotypic morphology of the face. The consequences of voluntary and involuntary signaling are significant for individuals (e.g., mate choice, occupational opportunities, sentencing decisions and so forth) and groups (e.g., voting preferences, effective within-culture and across culture communications). However, humans are highly adaptive social beings; like other social animals, humans can camouflage these involuntary morphology-based signals to optimize success within their ecological niche. We addressed the question of which specific facial movements modulate social perceptions of mental states ('thinking,' 'interested,' 'bored' and 'confused') and social traits ('attractiveness,' 'trustworthiness' and 'dominance') and also when in time the face transmits movements that rapidly signal approach avoidance vs. a wider range of social signals. We found, across the Western Caucasian (WC) and East Asian (EA) cultures, that early, simpler and, biologically rooted signals (wide opened eyes; wrinkled nose) support discrimination of elementary categories (e.g. approach/avoidance), whereas later more complex signals discriminate categories important for social interactions (e.g. facial expressions of emotion). Across WC and EA cultures, we also found common facial expressions of interest and boredom facilitating cross-cultural communication but a culture-specific expression of confusion that hinders cross-cultural communication--but not communication within culture. Finally, we found that facial movements of modulating the social attractiveness, trustworthiness and dominance of faces could trump the default social perceptions arising from their morphology.</gtr:description><gtr:exploitationPathways>The outcome of the research are generative models--i.e. dynamic mathematical models of facial expressions that can be mapped onto any 3D faces. Our models control how changes in the dynamics of the facial movements changes the perceived intensity of each emotion, mental state and social trait. Thus, our generative face models represent novel psychophysical laws for social sciences; these laws predict the perception of emotions, mental states and social traits on the basis of dynamic face identities. We are currently engaging with a Social Robotics company (Furhat Robotics http://www.furhatrobotics.com) to transfer our models onto their social robots to achieve realistic and culture-sensitive models of facial expressions.</gtr:exploitationPathways><gtr:id>0586E348-743D-4E80-B6EB-853C3D13098A</gtr:id><gtr:sectors><gtr:sector>Communities and Social Services/Policy,Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Healthcare,Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections,Security and Diplomacy</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/8A797F2D-218B-4A15-9BB1-ACA6E41B1661"><gtr:id>8A797F2D-218B-4A15-9BB1-ACA6E41B1661</gtr:id><gtr:title>The Human Face as a Dynamic Tool for Social Communication.</gtr:title><gtr:parentPublicationTitle>Current biology : CB</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/110bc41dfcffc8cb1bb088e70bfb6c28"><gtr:id>110bc41dfcffc8cb1bb088e70bfb6c28</gtr:id><gtr:otherNames>Jack RE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0960-9822</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/9FDC3A25-2D0C-4715-B68E-19B92C86D791"><gtr:id>9FDC3A25-2D0C-4715-B68E-19B92C86D791</gtr:id><gtr:title>Facial movements strategically camouflage involuntary social signals of face morphology.</gtr:title><gtr:parentPublicationTitle>Psychological science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/a65fc532df4d803bbe2d3078add87301"><gtr:id>a65fc532df4d803bbe2d3078add87301</gtr:id><gtr:otherNames>Gill D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0956-7976</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/48480BF8-77EF-4157-B132-9DA6B5907325"><gtr:id>48480BF8-77EF-4157-B132-9DA6B5907325</gtr:id><gtr:title>Dynamic facial expressions of emotion transmit an evolving hierarchy of signals over time.</gtr:title><gtr:parentPublicationTitle>Current biology : CB</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/110bc41dfcffc8cb1bb088e70bfb6c28"><gtr:id>110bc41dfcffc8cb1bb088e70bfb6c28</gtr:id><gtr:otherNames>Jack RE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0960-9822</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/A472ECDB-8906-4336-8E48-340A6581315D"><gtr:id>A472ECDB-8906-4336-8E48-340A6581315D</gtr:id><gtr:title>Toward a Social Psychophysics of Face Communication.</gtr:title><gtr:parentPublicationTitle>Annual review of psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/110bc41dfcffc8cb1bb088e70bfb6c28"><gtr:id>110bc41dfcffc8cb1bb088e70bfb6c28</gtr:id><gtr:otherNames>Jack RE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0066-4308</gtr:issn></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">ES/K00607X/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>E457FFDE-A4C1-4907-AE12-A394D95A3AE5</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Cognitive Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>E1AC33C6-9927-41AC-B23B-2EED8F593588</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Experimental Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>51E91F17-25CD-4FBD-917E-1BD82B72BDB9</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Mathematical &amp; Statistic Psych</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>56C9394E-9F52-4433-9619-6A14493473A8</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Social Psychology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>