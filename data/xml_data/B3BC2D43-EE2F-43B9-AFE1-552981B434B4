<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:department>Computing Sciences</gtr:department><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/2A5C593E-43C3-4E03-9530-990C61A80560"><gtr:id>2A5C593E-43C3-4E03-9530-990C61A80560</gtr:id><gtr:firstName>Graham</gtr:firstName><gtr:surname>Finlayson</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/4CD6A208-521C-4750-A420-C633E74928BB"><gtr:id>4CD6A208-521C-4750-A420-C633E74928BB</gtr:id><gtr:firstName>David</gtr:firstName><gtr:surname>Connah</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FI028455%2F1"><gtr:id>B3BC2D43-EE2F-43B9-AFE1-552981B434B4</gtr:id><gtr:title>SpectralEdge Image Visualisation</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I028455/1</gtr:grantReference><gtr:abstractText>A number of important imaging applications capture images from multiple sources, or channels. For example, satellite imaging systems capture images in both the visible and infra-red parts of the electromagnetic spectrum: the infra-red channels help to mitigate the effect of haze in obscuring the scene content, and help discriminate between features that are otherwise confounded in the visible-spectrum. The key problem, however, with capturing multiple images is that the extra information increases the cognitive load on the user (or computational load on automated algorithms), and can often hinder, rather than help, rapid decision making. As a result, a significant amount of effort has been put into finding ways to fuse multiple images into a single, easy to understand, image.SpectralEdge Image Visualisation aims to deliver a step change in the ability to visualise information from multiple sources, or channels, in a single image. The technology maximises the information that is conveyed while, at the same time, guaranteeing artefact free outputs. The technology is a platform technology, and has a wide scope for potential application. In this proposal, however, we target a specific problem: fusing colour images with infra-red for applications in the security and defence sectors, where there is an immediate and direct need for this technology. A key objective of this project will be to develop prototype software that is tailored to the needs of these end-users.</gtr:abstractText><gtr:potentialImpactText>SpectralEdge image-fusion technology improves the observability of features within a single image that is constructed by fusing together several images of the same scene derived from a variety of sources. The algorithm is fast, robust to noise, and produces artefact free images, in addition to maximizing the amount of information that is conveyed in the fusion process. This means that features that exist in the separate images are retained in the SpectralEdge output. The present proposal aims to develop the technology in two principal ways: to allow the method to work at video rates, and to cope with a potential image-registration problem (when the images from different sources are misaligned). From a technical point of view this will lead to demonstrator software that can be evaluated by our project partners (who are key suppliers in the defence and security sectors) on a specific problem: fusing colour images with infra-red images. Post-completion of this project we will raise funds or investment to develop commercial software/products that could enter the market as an embedded solution in products sold by these companies. By this route, the impact of our technology will be transmitted to the end-user. The business development component of the proposal will focus on reacting to user feedback from our demonstrator software, on developing new contacts within the defence and security sectors, and creating a business plan to secure further investment. These activities will help us to understand more about the markets, and most importantly, increase the potential avenues through which our technology can be distributed in the future. The end users in the defence sector will be the UK Ministry of Defence, and potentially other allied forces. The specific users will range from the dismounted solidier (in the case of weapons mounted display users) to satellite analysts charged with identifying targets or potential threats. The potential benefits are, of course, felt beyond those actually using the technology, and on to those they are bound to protect. While the focus of our initial work is the defence/security sectors, significantly, SpectralEdge is a platform technology. Indeed, in the long term, the variety of possible applications of the technology mean that the benefits could potentially be wide ranging, from improved medical diagnosis, to better monitoring of oil spills, and even to an improved visual experience for colour blind computer users.</gtr:potentialImpactText><gtr:fund><gtr:end>2011-08-31</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-12-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>63742</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our findings are the basis of the spin out company spectral Edge Ltd.

The company is productizing the research in various domains. Current foci include helping colour blind observers see better and unsing Near Infra Red to improve digital photographs.</gtr:description><gtr:firstYearOfImpact>2012</gtr:firstYearOfImpact><gtr:id>ADF42989-1A5B-4A8B-867A-FABA3724C4F2</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>Patent filed at national stage - publication number: WO2015004437A1.

All of us see the world similarly but different. This invention teaches how to retarget an image so that in a specific technical sense (colour matching and colorimetry) so that second person would see the same as the first. A specific embodiment of this invention is retargeting what a colour normal person would see so that a colour blind observer could experience the same visual sensation</gtr:description><gtr:id>D6623CED-0BEC-4835-A621-012E3D2B3051</gtr:id><gtr:impact>This patent is the foundation of 'eyeteq' a suite of platforms being developed to help colour blind people see better. This technology is close to commercial license and has been validated in an extensive third party study (by i2media)</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:patentId>WO2015004437A1</gtr:patentId><gtr:protection>Patent application published</gtr:protection><gtr:title>Image Processing Method and System</gtr:title></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>A well founded method was engineered to allow fusion of multiple images to arrive at a fused image which conveys the information from all the individual component images.</gtr:description><gtr:exploitationPathways>We are commercializing through spectraledge ltd</gtr:exploitationPathways><gtr:id>12EC67CB-07C0-4811-9245-8CE943D8AE93</gtr:id><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Agriculture, Food and Drink,Communities and Social Services/Policy,Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Healthcare</gtr:sector></gtr:sectors><gtr:url>http://www.spectraledge.co.uk</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs><gtr:spinOutOutput><gtr:companyName>Spectral Edge Ltd</gtr:companyName><gtr:description>Spectral Edge Ltd is a company operating in the area of Image Fusion. Its aim is to develop and further extend the image fusion work developed in I028455 and E012248. While the technology developed can be applied to many domains (remote sensing, defense, medical imaging) the current and medium term focus of the company is in visual accessibility (see eyeteq.com), fusion for enhanced photography and surveillance.

The company is based in Cambridge, has to date raised about &amp;pound;2 million in venture capital and has 6 FTE employees.</gtr:description><gtr:id>8DD0F6D7-FE48-48C7-A19C-12B257F686D7</gtr:id><gtr:impact>The company is in an early technological development stage and we expect commercialization to begin in 2016. However, the visual accessibility s/w - for colour deficient observers - has been validated in a large third party trial (run by i2media).</gtr:impact><gtr:url>http://www.spectraledge.co.uk</gtr:url><gtr:yearCompanyFormed>2012</gtr:yearCompanyFormed></gtr:spinOutOutput></gtr:spinOutOutputs></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/715E9232-8110-424B-A49D-D5967CD78E7B"><gtr:id>715E9232-8110-424B-A49D-D5967CD78E7B</gtr:id><gtr:title>Lookup-table-based gradient field reconstruction.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/77BC2EF6-9D7C-48DA-B319-D09D5F535C1A"><gtr:id>77BC2EF6-9D7C-48DA-B319-D09D5F535C1A</gtr:id><gtr:title>Spectral edge: gradient-preserving spectral mapping for image fusion.</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America. A, Optics, image science, and vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/b94c2cc34bea7d5eb433bd140e91d732"><gtr:id>b94c2cc34bea7d5eb433bd140e91d732</gtr:id><gtr:otherNames>Connah D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/4972FBBE-7E15-473E-8E77-5A33DAF79643"><gtr:id>4972FBBE-7E15-473E-8E77-5A33DAF79643</gtr:id><gtr:title>Spectral Edge image Fusion: Theory and Applications</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/9a4adf2ef4783966abec40e1148c5d94"><gtr:id>9a4adf2ef4783966abec40e1148c5d94</gtr:id><gtr:otherNames>Connah DR</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/49070C68-9906-4082-8A9E-3ED26DD93D40"><gtr:id>49070C68-9906-4082-8A9E-3ED26DD93D40</gtr:id><gtr:title>Iterative Spectral Edge Image Fusion</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/5E5AC7EE-AB8B-4C55-B48C-E5F3384F68E5"><gtr:id>5E5AC7EE-AB8B-4C55-B48C-E5F3384F68E5</gtr:id><gtr:title>RGB-NIR colour image fusion: metric and psychophysical experiment</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/f6da54c65332d2ea64722d5b195276b6"><gtr:id>f6da54c65332d2ea64722d5b195276b6</gtr:id><gtr:otherNames>Hayes AE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/F757B8F2-0E09-4E62-BB57-9B0F99B8639D"><gtr:id>F757B8F2-0E09-4E62-BB57-9B0F99B8639D</gtr:id><gtr:title>Pop Image Fusion - Derivative Domain Image Fusion Without Reintegration</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/1C4C523F-5DCB-43CC-AB38-476415AACAD8"><gtr:id>1C4C523F-5DCB-43CC-AB38-476415AACAD8</gtr:id><gtr:title>Computer Vision - ECCV 2014</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/b94c2cc34bea7d5eb433bd140e91d732"><gtr:id>b94c2cc34bea7d5eb433bd140e91d732</gtr:id><gtr:otherNames>Connah D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:isbn>978-3-319-10601-4</gtr:isbn></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I028455/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>